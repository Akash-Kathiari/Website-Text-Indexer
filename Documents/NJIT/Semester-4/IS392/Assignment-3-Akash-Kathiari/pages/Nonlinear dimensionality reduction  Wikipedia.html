<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Nonlinear dimensionality reduction - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Nonlinear_dimensionality_reduction","wgTitle":"Nonlinear dimensionality reduction","wgCurRevisionId":886008276,"wgRevisionId":886008276,"wgArticleId":309261,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","Dimension reduction"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Nonlinear_dimensionality_reduction","wgRelevantArticleId":309261,"wgRequestId":"XHzHtQpAAEEAAJFac2cAAABR","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"Manifold_learning","wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsReferencePreviews":false,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFIsPageContentModelEditable":true,"wgMFEnableFontChanger":true,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgInternalRedirectTargetUrl":"/wiki/Nonlinear_dimensionality_reduction#Manifold_learning_algorithms","wgWikibaseItemId":"Q7049464","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true,"oresWikiId":"enwiki","oresBaseUrl":"http://ores.discovery.wmnet:8081/","oresApiVersion":3});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["mediawiki.action.view.redirect","ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.33.0-wmf.19"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/f/fd/Lle_hlle_swissroll.png"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Manifold_learning_algorithms"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Manifold_learning_algorithms"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Nonlinear_dimensionality_reduction rootpage-Nonlinear_dimensionality_reduction skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Nonlinear dimensionality reduction</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"><span class="mw-redirectedfrom">&#160;&#160;(Redirected from <a href="/w/index.php?title=Manifold_learning&amp;redirect=no" class="mw-redirect" title="Manifold learning">Manifold learning</a>)</span></div>
				<div id="jump-to-nav"></div>				<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
				<a class="mw-jump-link" href="#p-search">Jump to search</a>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Summary of algorithms for nonlinear dimensionality reduction</div>
<p><a href="/wiki/High-dimensional" class="mw-redirect" title="High-dimensional">High-dimensional</a> data, meaning data that requires more than two or three dimensions to represent, can be  <a href="/wiki/Curse_of_Dimensionality" class="mw-redirect" title="Curse of Dimensionality">difficult to interpret</a>. One approach to simplification is to assume that the data of interest lie on an <a href="/wiki/Embedding" title="Embedding">embedded</a> non-linear <a href="/wiki/Manifold" title="Manifold">manifold</a> within the <a href="/wiki/Higher-dimensional_space" class="mw-redirect" title="Higher-dimensional space">higher-dimensional space</a>. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="/wiki/File:Lle_hlle_swissroll.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Lle_hlle_swissroll.png/300px-Lle_hlle_swissroll.png" decoding="async" width="300" height="234" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Lle_hlle_swissroll.png/450px-Lle_hlle_swissroll.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Lle_hlle_swissroll.png/600px-Lle_hlle_swissroll.png 2x" data-file-width="906" data-file-height="708" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Lle_hlle_swissroll.png" class="internal" title="Enlarge"></a></div>Top-left: a 3D dataset of 1000 points in a spiraling band (a.k.a. the <a href="/wiki/Swiss_roll" title="Swiss roll">Swiss roll</a>) with a rectangular hole in the middle. Top-right: the original 2D manifold used to generate the 3D dataset. Bottom left and right: 2D recoveries of the manifold respectively using the <a href="/wiki/Nonlinear_dimensionality_reduction#Locally-linear_embedding" title="Nonlinear dimensionality reduction">LLE</a> and <a href="/wiki/Nonlinear_dimensionality_reduction#Hessian_Locally-Linear_Embedding_(Hessian_LLE)" title="Nonlinear dimensionality reduction">Hessian LLE</a> algorithms as implemented by the Modular Data Processing toolkit.</div></div></div>
<p>Below is a summary of some of the important algorithms from the history of <b>manifold learning</b> and <b>nonlinear dimensionality reduction</b> (NLDR).<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> Many of these non-linear <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> methods are related to the <a href="#Related_Linear_Decomposition_Methods">linear methods listed below</a>. Non-linear methods can be broadly classified into two groups: those that provide a mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa), and those that just give a visualisation.  In the context of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, mapping methods may be viewed as a preliminary <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a> step, after which <a href="/wiki/Pattern_recognition#Algorithms" title="Pattern recognition">pattern recognition algorithms</a> are applied. Typically those that just give a visualisation are based on proximity data – that is, <a href="/wiki/Distance" title="Distance">distance</a> measurements.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Related_Linear_Decomposition_Methods"><span class="tocnumber">1</span> <span class="toctext">Related Linear Decomposition Methods</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Applications_of_NLDR"><span class="tocnumber">2</span> <span class="toctext">Applications of NLDR</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Manifold_learning_algorithms"><span class="tocnumber">3</span> <span class="toctext">Manifold learning algorithms</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Isomap"><span class="tocnumber">3.1</span> <span class="toctext">Isomap</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Fast_Manifold_Learning:[5]_SDD_Maps_for_Sub-linear_Time_Fast_Manifold_Learning"><span class="tocnumber">3.2</span> <span class="toctext">Fast Manifold Learning:<sup>&#91;5&#93;</sup> SDD Maps for Sub-linear Time Fast Manifold Learning</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Locally-linear_embedding"><span class="tocnumber">3.3</span> <span class="toctext">Locally-linear embedding</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Laplacian_eigenmaps"><span class="tocnumber">3.4</span> <span class="toctext">Laplacian eigenmaps</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Sammon&#39;s_mapping"><span class="tocnumber">3.5</span> <span class="toctext">Sammon's mapping</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Self-organizing_map"><span class="tocnumber">3.6</span> <span class="toctext">Self-organizing map</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Principal_curves_and_manifolds"><span class="tocnumber">3.7</span> <span class="toctext">Principal curves and manifolds</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Autoencoders"><span class="tocnumber">3.8</span> <span class="toctext">Autoencoders</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Gaussian_process_latent_variable_models"><span class="tocnumber">3.9</span> <span class="toctext">Gaussian process latent variable models</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Contagion_maps"><span class="tocnumber">3.10</span> <span class="toctext">Contagion maps</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Curvilinear_component_analysis"><span class="tocnumber">3.11</span> <span class="toctext">Curvilinear component analysis</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Curvilinear_distance_analysis"><span class="tocnumber">3.12</span> <span class="toctext">Curvilinear distance analysis</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Diffeomorphic_dimensionality_reduction"><span class="tocnumber">3.13</span> <span class="toctext">Diffeomorphic dimensionality reduction</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Kernel_principal_component_analysis"><span class="tocnumber">3.14</span> <span class="toctext">Kernel principal component analysis</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Manifold_alignment"><span class="tocnumber">3.15</span> <span class="toctext">Manifold alignment</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Diffusion_maps"><span class="tocnumber">3.16</span> <span class="toctext">Diffusion maps</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Hessian_Locally-Linear_Embedding_(Hessian_LLE)"><span class="tocnumber">3.17</span> <span class="toctext">Hessian Locally-Linear Embedding (Hessian LLE)</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Modified_Locally-Linear_Embedding_(MLLE)"><span class="tocnumber">3.18</span> <span class="toctext">Modified Locally-Linear Embedding (MLLE)</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="#Relational_perspective_map"><span class="tocnumber">3.19</span> <span class="toctext">Relational perspective map</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#Local_tangent_space_alignment"><span class="tocnumber">3.20</span> <span class="toctext">Local tangent space alignment</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="#Local_multidimensional_scaling"><span class="tocnumber">3.21</span> <span class="toctext">Local multidimensional scaling</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Maximum_variance_unfolding"><span class="tocnumber">3.22</span> <span class="toctext">Maximum variance unfolding</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Nonlinear_PCA"><span class="tocnumber">3.23</span> <span class="toctext">Nonlinear PCA</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Data-driven_high-dimensional_scaling"><span class="tocnumber">3.24</span> <span class="toctext">Data-driven high-dimensional scaling</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#Manifold_sculpting"><span class="tocnumber">3.25</span> <span class="toctext">Manifold sculpting</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#t-distributed_stochastic_neighbor_embedding"><span class="tocnumber">3.26</span> <span class="toctext">t-distributed stochastic neighbor embedding</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="#RankVisu"><span class="tocnumber">3.27</span> <span class="toctext">RankVisu</span></a></li>
<li class="toclevel-2 tocsection-31"><a href="#Topologically_constrained_isometric_embedding"><span class="tocnumber">3.28</span> <span class="toctext">Topologically constrained isometric embedding</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-32"><a href="#Methods_based_on_proximity_matrices"><span class="tocnumber">4</span> <span class="toctext">Methods based on proximity matrices</span></a></li>
<li class="toclevel-1 tocsection-33"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-34"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-35"><a href="#External_links"><span class="tocnumber">7</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Related_Linear_Decomposition_Methods">Related Linear Decomposition Methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=1" title="Edit section: Related Linear Decomposition Methods">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a> (ICA).</li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a> (PCA) (also called <a href="/wiki/Karhunen%E2%80%93Lo%C3%A8ve_transform" class="mw-redirect" title="Karhunen–Loève transform">Karhunen&#8211;Loève transform</a> &#8212; KLT).</li>
<li><a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">Singular value decomposition</a> (SVD).</li>
<li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a>.</li></ul>
<h2><span class="mw-headline" id="Applications_of_NLDR">Applications of NLDR</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=2" title="Edit section: Applications of NLDR">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Consider a dataset represented as a matrix (or a database table), such that each row represents a set of attributes (or features or dimensions) that describe a particular instance of something. If the number of attributes is large, then the space of unique possible rows is exponentially large. Thus, the larger the dimensionality, the more difficult it becomes to sample the space. This causes many problems. Algorithms that operate on high-dimensional data tend to have a very high time complexity. Many machine learning algorithms, for example, struggle with high-dimensional data. This has become known as the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>. Reducing data into fewer dimensions often makes analysis algorithms more efficient, and can help machine learning algorithms make more accurate predictions.
</p><p>Humans often have difficulty comprehending data in many dimensions. Thus, reducing data to a small number of dimensions is useful for visualization purposes.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:502px;"><a href="/wiki/File:Nldr.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/9c/Nldr.jpg/500px-Nldr.jpg" decoding="async" width="500" height="227" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/en/9/9c/Nldr.jpg 1.5x" data-file-width="584" data-file-height="265" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Nldr.jpg" class="internal" title="Enlarge"></a></div>Plot of the two-dimensional points that results from using a NLDR algorithm. In this case, Manifold Sculpting used to reduce the data into just two dimensions (rotation and scale).</div></div></div>
<p>The reduced-dimensional representations of data are often referred to as "intrinsic variables". This description implies that these are the values from which the data was produced. For example, consider a dataset that contains images of a letter 'A', which has been scaled and rotated by varying amounts. Each image has 32x32 pixels. Each image can be represented as a vector of 1024 pixel values. Each row is a sample on a two-dimensional manifold in 1024-dimensional space (a <a href="/wiki/Hamming_space" title="Hamming space">Hamming space</a>). The intrinsic dimensionality is two, because two variables (rotation and scale) were varied in order to produce the data. Information about the shape or look of a letter 'A' is not part of the intrinsic variables because it is the same in every instance. Nonlinear dimensionality reduction will discard the correlated information (the letter 'A') and recover only the varying information (rotation and scale). The image to the right shows sample images from this dataset (to save space, not all input images are shown), and a plot of the two-dimensional points that results from using a NLDR algorithm (in this case, Manifold Sculpting was used) to reduce the data into just two dimensions.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:234px;"><a href="/wiki/File:Letters_pca.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/e/e1/Letters_pca.png" decoding="async" width="232" height="232" class="thumbimage" data-file-width="232" data-file-height="232" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Letters_pca.png" class="internal" title="Enlarge"></a></div>PCA (a linear dimensionality reduction algorithm) is used to reduce this same dataset into two dimensions, the resulting values are not so well organized.</div></div></div>
<p>By comparison, if <a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a>, which is a linear dimensionality reduction algorithm, is used to reduce this same dataset into two dimensions, the resulting values are not so well organized. This demonstrates that the high-dimensional vectors (each representing a letter 'A') that sample this manifold vary in a non-linear manner.
</p><p>It should be apparent, therefore, that NLDR has several applications in the field of computer-vision. For example, consider a robot that uses a camera to navigate in a closed static environment. The images obtained by that camera can be considered to be samples on a manifold in high-dimensional space, and the intrinsic variables of that manifold will represent the robot's position and orientation. This utility is not limited to robots. <a href="/wiki/Dynamical_systems" class="mw-redirect" title="Dynamical systems">Dynamical systems</a>, a more general class of systems, which includes robots, are defined in terms of a manifold. Active research in NLDR seeks to unfold the observation manifolds associated with dynamical systems to develop techniques for modeling such systems and enable them to operate autonomously.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Manifold_learning_algorithms">Manifold learning algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=3" title="Edit section: Manifold learning algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Some of the more prominent manifold learning algorithms are listed below. An algorithm may learn an <i>internal model</i> of the data, which can be used to map points unavailable at training time into the embedding in a process often called out-of-sample extension.
</p>
<h3><span class="mw-headline" id="Isomap">Isomap</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=4" title="Edit section: Isomap">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Isomap" title="Isomap">Isomap</a><sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> is a combination of the <a href="/wiki/Floyd%E2%80%93Warshall_algorithm" title="Floyd–Warshall algorithm">Floyd–Warshall algorithm</a> with classic <a href="/wiki/Multidimensional_Scaling" class="mw-redirect" title="Multidimensional Scaling">Multidimensional Scaling</a>. Classic Multidimensional Scaling (MDS) takes a matrix of pair-wise distances between all points and computes a position for each point.  Isomap assumes that the pair-wise distances are only known between neighboring points, and uses the Floyd–Warshall algorithm to compute the pair-wise distances between all other points. This effectively estimates the full matrix of pair-wise <a href="/wiki/Geodesic_distance" class="mw-redirect" title="Geodesic distance">geodesic distances</a> between all of the points. Isomap then uses classic MDS to compute the reduced-dimensional positions of all the points. Landmark-Isomap is a variant of this algorithm that uses landmarks to increase speed, at the cost of some accuracy.
</p>
<h3><span id="Fast_Manifold_Learning:.5B5.5D_SDD_Maps_for_Sub-linear_Time_Fast_Manifold_Learning"></span><span class="mw-headline" id="Fast_Manifold_Learning:[5]_SDD_Maps_for_Sub-linear_Time_Fast_Manifold_Learning">Fast Manifold Learning:<sup id="cite_ref-:0_5-0" class="reference"><a href="#cite_note-:0-5">&#91;5&#93;</a></sup> SDD Maps for Sub-linear Time Fast Manifold Learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=5" title="Edit section: Fast Manifold Learning:[5] SDD Maps for Sub-linear Time Fast Manifold Learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>SDD Maps <sup id="cite_ref-:0_5-1" class="reference"><a href="#cite_note-:0-5">&#91;5&#93;</a></sup> are fast manifold learning algorithms obtained by formulating the problem as a Laplacian Linear System. This is done by replacing the quadratic weighted orthonormality constraints used in popular manifold learning techniques to prevent trivial solutions with a linear constraint that prevents the same. This converts the quadratically constrained quadratic optimization problem into a simpler formulation that is a linearly constrained quadratic optimization problem. Furthermore, in the case of SDD Maps, this problem is equivalent to solving a symmetric diagonally dominant (SDD) linear system which can be solved very fast using Spielman and Teng solvers for Laplacian Linear Systems. The work by Spielman/Teng on such solvers had won a Godel prize, and found many applications later on such as the SDD Maps.
</p>
<h3><span class="mw-headline" id="Locally-linear_embedding">Locally-linear embedding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=6" title="Edit section: Locally-linear embedding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/w/index.php?title=Locally-Linear_Embedding&amp;action=edit&amp;redlink=1" class="new" title="Locally-Linear Embedding (page does not exist)">Locally-Linear Embedding</a> (LLE)<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> was presented at approximately the same time as Isomap. It has several advantages over Isomap, including faster optimization when implemented to take advantage of <a href="/wiki/Sparse_matrix" title="Sparse matrix">sparse matrix</a> algorithms, and better results with many problems. LLE also begins by finding a set of the nearest neighbors of each point. It then computes a set of weights for each point that best describes the point as a linear combination of its neighbors. Finally, it uses an eigenvector-based optimization technique to find the low-dimensional embedding of points, such that each point is still described with the same linear combination of its neighbors. LLE tends to handle non-uniform sample densities poorly because there is no fixed unit to prevent the weights from drifting as various regions differ in sample densities. LLE has no internal model.
</p><p>LLE computes the barycentric coordinates of a point <i>X</i><sub><i>i</i></sub> based on its neighbors <i>X</i><sub><i>j</i></sub>. The original point is reconstructed by a linear combination, given by the weight matrix <i>W</i><sub><i>ij</i></sub>, of its neighbors. The reconstruction error is given by the cost function <i>E</i>(<i>W</i>).
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E(W)=\sum _{i}|{\mathbf {X} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {X} _{j}}|}^{\mathsf {2}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo stretchy="false">(</mo>
        <mi>W</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">X</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>&#x2212;<!-- − --></mo>
            <munder>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </munder>
            <mrow class="MJX-TeXAtom-ORD">
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">X</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">|</mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mn mathvariant="sans-serif">2</mn>
            </mrow>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E(W)=\sum _{i}|{\mathbf {X} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {X} _{j}}|}^{\mathsf {2}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fc0bdb0b95b0a2d60d56e57add4d00cb1ed1da27" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:31.779ex; height:6.343ex;" alt="E(W)=\sum _{i}|{\mathbf {X} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {X} _{j}}|}^{\mathsf {2}}"/></span></dd></dl>
<p>The weights <i>W</i><sub><i>ij</i></sub> refer to the amount of contribution the point <i>X</i><sub><i>j</i></sub> has while reconstructing the point <i>X</i><sub><i>i</i></sub>. The cost function is minimized under two constraints:
(a) Each data point <i>X</i><sub><i>i</i></sub> is reconstructed only from its neighbors, thus enforcing <i>W</i><sub><i>ij</i></sub> to be zero if point <i>X</i><sub><i>j</i></sub> is not a neighbor of the point <i>X</i><sub><i>i</i></sub> and 
(b) The sum of every row of the weight matrix equals 1.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{j}{\mathbf {W} _{ij}}=1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">W</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
              <mi>j</mi>
            </mrow>
          </msub>
        </mrow>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{j}{\mathbf {W} _{ij}}=1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/281e3fb718c89cd647f0e97b944b2256aa03c9a2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:12.243ex; height:5.843ex;" alt="\sum _{j}{\mathbf {W} _{ij}}=1"/></span></dd></dl>
<p>The original data points are collected in a <i>D</i> dimensional space and the goal of the algorithm is to reduce the dimensionality to <i>d</i> such that <i>D</i> &gt;&gt; <i>d</i>. The same weights <i>W</i><sub><i>ij</i></sub> that reconstructs the <i>i</i>th data point in the <i>D</i> dimensional space will be used to reconstruct the same point in the lower <i>d</i> dimensional space. A neighborhood preserving map is created based on this idea. Each point X<sub>i</sub> in the <i>D</i> dimensional space is mapped onto a point Y<sub>i</sub> in the <i>d</i> dimensional space by minimizing the cost function
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C(Y)=\sum _{i}|{\mathbf {Y} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {Y} _{j}}|}^{\mathsf {2}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo stretchy="false">(</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">Y</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>&#x2212;<!-- − --></mo>
            <munder>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </munder>
            <mrow class="MJX-TeXAtom-ORD">
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">Y</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">|</mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mn mathvariant="sans-serif">2</mn>
            </mrow>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C(Y)=\sum _{i}|{\mathbf {Y} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {Y} _{j}}|}^{\mathsf {2}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07e4d9614a270cf1d338095999935621139f8854" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:31.108ex; height:6.343ex;" alt="C(Y)=\sum _{i}|{\mathbf {Y} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {Y} _{j}}|}^{\mathsf {2}}"/></span></dd></dl>
<p>In this cost function, unlike the previous one, the weights W<sub>ij</sub> are kept fixed and the minimization is done on the points Y<sub>i</sub> to optimize the coordinates. This minimization problem can be solved by solving a sparse <i>N</i> X <i>N</i> <a href="/wiki/Eigendecomposition_of_a_matrix" title="Eigendecomposition of a matrix">eigen value problem</a> (<i>N</i> being the number of data points), whose bottom <i>d</i> nonzero eigen vectors provide an orthogonal set of coordinates. Generally the data points are reconstructed from <i>K</i> nearest neighbors, as measured by <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>. For such an implementation the algorithm has only one free parameter <i>K,</i> which can be chosen by cross validation.
</p>
<h3><span class="mw-headline" id="Laplacian_eigenmaps">Laplacian eigenmaps</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=7" title="Edit section: Laplacian eigenmaps">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Manifold_regularization" title="Manifold regularization">Manifold regularization</a></div>
<p>Laplacian Eigenmaps<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup> uses spectral techniques to perform dimensionality reduction. This technique relies on the basic assumption that the data lies in a low-dimensional manifold in a high-dimensional space.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup>  This algorithm cannot embed out-of-sample points, but techniques based on <a href="/wiki/Reproducing_kernel_Hilbert_space" title="Reproducing kernel Hilbert space">Reproducing kernel Hilbert space</a> regularization exist for adding this capability.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup>  Such techniques can be applied to other nonlinear dimensionality reduction algorithms as well.
</p><p>Traditional techniques like principal component analysis do not consider the intrinsic geometry of the data. Laplacian eigenmaps builds a graph from neighborhood information of the data set. Each data point serves as a node on the graph and connectivity between nodes is governed by the proximity of neighboring points (using e.g. the <a href="/wiki/K-nearest_neighbor_algorithm" class="mw-redirect" title="K-nearest neighbor algorithm">k-nearest neighbor algorithm</a>). The graph thus generated can be considered as a discrete approximation of the low-dimensional manifold in the high-dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low-dimensional space, preserving local distances. The eigenfunctions of the <a href="/wiki/Laplace%E2%80%93Beltrami_operator" title="Laplace–Beltrami operator">Laplace–Beltrami operator</a> on the manifold serve as the embedding dimensions, since under mild conditions this operator has a countable spectrum that is a basis for square integrable functions on the manifold (compare to <a href="/wiki/Fourier_series" title="Fourier series">Fourier series</a> on the unit circle manifold).  Attempts to place Laplacian eigenmaps on solid theoretical ground have met with some success, as under certain nonrestrictive assumptions, the graph Laplacian matrix has been shown to converge to the Laplace–Beltrami operator as the number of points goes to infinity.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup> Matlab code for Laplacian Eigenmaps can be found in algorithms<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup> and the PhD thesis of Belkin can be found at the <a href="/wiki/Ohio_State_University" title="Ohio State University">Ohio State University</a>.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup>
</p><p>In classification applications, low dimension manifolds can be used to model data classes which can be defined from sets of observed instances. Each observed instance can be described by two independent factors termed ’content’ and ’style’, where ’content’ is the invariant factor related to the essence of the class and ’style’ expresses variations in that class between instances.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup> Unfortunately, Laplacian Eigenmaps may fail to produce a coherent representation of a class of interest when training data consist of instances varying significantly in terms of style.<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup> In the case of classes which are represented by multivariate sequences, Structural Laplacian Eigenmaps has been proposed to overcome this issue by adding additional constraints within the Laplacian Eigenmaps neighborhood information graph to better reflect the intrinsic structure of the class.<sup id="cite_ref-ReferenceB_15-0" class="reference"><a href="#cite_note-ReferenceB-15">&#91;15&#93;</a></sup> More specifically, the graph is used to encode both the sequential structure of the multivariate sequences and, to minimise stylistic variations, proximity between data points of different sequences or even within a sequence, if it contains repetitions. Using <a href="/wiki/Dynamic_time_warping" title="Dynamic time warping">dynamic time warping</a>, proximity is detected by finding correspondences between and within sections of the multivariate sequences that exhibit high similarity. Experiments conducted on <a href="/wiki/Vision-based_activity_recognition" class="mw-redirect" title="Vision-based activity recognition">vision-based activity recognition</a>, object orientation classification and human 3D pose recovery applications have demonstrate the added value of Structural Laplacian Eigenmaps when dealing with multivariate sequence data.<sup id="cite_ref-ReferenceB_15-1" class="reference"><a href="#cite_note-ReferenceB-15">&#91;15&#93;</a></sup> An extension of Structural Laplacian Eigenmaps, Generalized Laplacian Eigenmaps led to the generation of manifolds where one of the dimensions specifically represents variations in style. This has proved particularly valuable in applications such as tracking of the human articulated body and silhouette extraction.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup>
</p>
<h3><span id="Sammon.27s_mapping"></span><span class="mw-headline" id="Sammon's_mapping">Sammon's mapping</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=8" title="Edit section: Sammon&#039;s mapping">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Sammon%27s_mapping" class="mw-redirect" title="Sammon&#39;s mapping">Sammon's mapping</a> is one of the first and most popular NLDR techniques.
</p>
<div class="thumb tleft"><div class="thumbinner" style="width:202px;"><a href="/wiki/File:SOMsPCA.PNG" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/bb/SOMsPCA.PNG/200px-SOMsPCA.PNG" decoding="async" width="200" height="132" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/bb/SOMsPCA.PNG/300px-SOMsPCA.PNG 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/bb/SOMsPCA.PNG/400px-SOMsPCA.PNG 2x" data-file-width="607" data-file-height="400" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:SOMsPCA.PNG" class="internal" title="Enlarge"></a></div>Approximation of a principal curve by one-dimensional <a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a> (a <a href="/wiki/Broken_line" class="mw-redirect" title="Broken line">broken line</a> with red squares, 20 nodes). The first <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component</a> is presented by a blue straight line. Data points are the small grey circles. For PCA, the <a href="/wiki/Fraction_of_variance_unexplained" title="Fraction of variance unexplained">Fraction of variance unexplained</a> in this example is 23.23%, for SOM it is 6.86%.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup></div></div></div>
<h3><span class="mw-headline" id="Self-organizing_map">Self-organizing map</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=9" title="Edit section: Self-organizing map">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a> (SOM, also called <i>Kohonen map</i>) and its probabilistic variant <a href="/wiki/Generative_topographic_mapping" class="mw-redirect" title="Generative topographic mapping">generative topographic mapping</a> (GTM) use a point representation in the embedded space to form a <a href="/wiki/Latent_variable_model" title="Latent variable model">latent variable model</a> based on a non-linear mapping from the embedded space to the high-dimensional space.<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup> These techniques are related to work on <a href="/w/index.php?title=Density_networks&amp;action=edit&amp;redlink=1" class="new" title="Density networks (page does not exist)">density networks</a>, which also are based around the same probabilistic model.
</p>
<h3><span class="mw-headline" id="Principal_curves_and_manifolds">Principal curves and manifolds</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=10" title="Edit section: Principal curves and manifolds">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="/wiki/File:SlideQualityLife.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/48/SlideQualityLife.png/300px-SlideQualityLife.png" decoding="async" width="300" height="230" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/48/SlideQualityLife.png/450px-SlideQualityLife.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/48/SlideQualityLife.png/600px-SlideQualityLife.png 2x" data-file-width="1164" data-file-height="892" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:SlideQualityLife.png" class="internal" title="Enlarge"></a></div>Application of principal curves: Nonlinear quality of life index.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> Points represent data of the <a href="/wiki/United_Nations" title="United Nations">UN</a> 171 countries in 4-dimensional space formed by the values of 4 indicators: <a href="/wiki/Gross_domestic_product" title="Gross domestic product">gross product per capita</a>, <a href="/wiki/Life_expectancy" title="Life expectancy">life expectancy</a>, <a href="/wiki/Infant_mortality" title="Infant mortality">infant mortality</a>, <a href="/wiki/Tuberculosis" title="Tuberculosis">tuberculosis</a> incidence. Different forms and colors correspond to various geographical locations. Red bold line represents the <b>principal curve</b>, approximating the dataset. This principal curve was produced by the method of <a href="/wiki/Elastic_map" title="Elastic map">elastic map</a>. Software is available for free non-commercial use.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup><sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup></div></div></div>
<p><b><a href="/w/index.php?title=Principal_curve&amp;action=edit&amp;redlink=1" class="new" title="Principal curve (page does not exist)">Principal curves</a> and manifolds</b> give the natural geometric framework for nonlinear dimensionality reduction and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold, and by encoding using standard geometric projection onto the manifold. This approach was proposed by <a href="/wiki/Trevor_Hastie" title="Trevor Hastie">Trevor Hastie</a> in his thesis (1984)<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> and developed further by many authors.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup>
How to define the "simplicity" of the manifold is problem-dependent, however, it is commonly measured by the intrinsic dimensionality and/or the smoothness of the manifold. Usually, the principal manifold is defined as a solution to an optimization problem. The objective function includes a quality of data approximation and some penalty terms for the bending of the manifold. The popular initial approximations are generated by linear PCA, Kohonen's SOM or autoencoders. The <a href="/wiki/Elastic_map" title="Elastic map">elastic map</a> method provides the <a href="/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> for principal <a href="/wiki/Manifold_learning" class="mw-redirect" title="Manifold learning">manifold learning</a> with minimization of quadratic energy functional at the "maximization" step.
</p>
<h3><span class="mw-headline" id="Autoencoders">Autoencoders</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=11" title="Edit section: Autoencoders">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An <a href="/wiki/Autoencoder" title="Autoencoder">autoencoder</a> is a feed-forward <a href="/wiki/Neural_network" title="Neural network">neural network</a> which is trained to approximate the identity function. That is, it is trained to map from a vector of values to the same vector. When used for dimensionality reduction purposes, one of the hidden layers in the network is limited to contain only a small number of network units. Thus, the network must learn to encode the vector into a small number of dimensions and then decode it back into the original space. Thus, the first half of the network is a model which maps from high to low-dimensional space, and the second half maps from low to high-dimensional space. Although the idea of autoencoders is quite old, training of deep autoencoders has only recently become possible through the use of <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machines</a> and stacked denoising autoencoders. Related to autoencoders is the <a href="/w/index.php?title=NeuroScale&amp;action=edit&amp;redlink=1" class="new" title="NeuroScale (page does not exist)">NeuroScale</a> algorithm, which uses stress functions inspired by <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a> and <a href="/wiki/Sammon_mapping" title="Sammon mapping">Sammon mappings</a> (see below) to learn a non-linear mapping from the high-dimensional to the embedded space. The mappings in NeuroScale are based on <a href="/wiki/Radial_basis_function_network" title="Radial basis function network">radial basis function networks</a>. Another usage of a neural network for dimensionality reduction is make it learn the tangent planes in the data.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Gaussian_process_latent_variable_models">Gaussian process latent variable models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=12" title="Edit section: Gaussian process latent variable models">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/w/index.php?title=Gaussian_process_latent_variable_model&amp;action=edit&amp;redlink=1" class="new" title="Gaussian process latent variable model (page does not exist)">Gaussian process latent variable models</a> (GPLVM)<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup> are probabilistic dimensionality reduction methods that use Gaussian Processes (GPs) to find a lower dimensional non-linear embedding of high dimensional data. They are an extension of the Probabilistic formulation of PCA. The model is defined probabilistically and the latent variables are then marginalized and parameters are obtained by maximizing the likelihood. Like kernel PCA they use a kernel function to form a non linear mapping (in the form of a <a href="/wiki/Gaussian_process" title="Gaussian process">Gaussian process</a>). However, in the GPLVM the mapping is from the embedded(latent) space to the data space (like density networks and GTM) whereas in kernel PCA it is in the opposite direction. It was originally proposed for visualization of high dimensional data but has been extended to construct a shared manifold model between two observation spaces.
GPLVM and its many variants have been proposed specially for human motion modeling, e.g., back constrained GPLVM, GP dynamic model (GPDM), balanced GPDM (B-GPDM) and topologically constrained GPDM. To capture the coupling effect of the pose and gait manifolds in the gait analysis, a multi-layer joint gait-pose manifolds was proposed.<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Contagion_maps">Contagion maps</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=13" title="Edit section: Contagion maps">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Contagion maps use multiple contagions on a network to map the nodes as a point cloud.<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup> In the case of the <a href="/wiki/Global_cascades_model" title="Global cascades model">Global cascades model</a> the speed of the spread can be adjusted with the threshold parameter <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle t\in [0,1]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>t</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t\in [0,1]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/31a5c18739ff04858eecc8fec2f53912c348e0e5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.333ex; height:2.843ex;" alt="{\displaystyle t\in [0,1]}"/></span>. For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle t=0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>t</mi>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t=0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/43469ec032d858feae5aa87029e22eaaf0109e9c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.101ex; height:2.176ex;" alt=" t=0 "/></span> the contagion map is equivalent to the <a href="/wiki/Isomap" title="Isomap">Isomap</a> algorithm.
</p>
<h3><span class="mw-headline" id="Curvilinear_component_analysis">Curvilinear component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=14" title="Edit section: Curvilinear component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/w/index.php?title=Curvilinear_component_analysis&amp;action=edit&amp;redlink=1" class="new" title="Curvilinear component analysis (page does not exist)">Curvilinear component analysis</a> (CCA)<sup id="cite_ref-Demart_28-0" class="reference"><a href="#cite_note-Demart-28">&#91;28&#93;</a></sup> looks for the configuration of points in the output space that preserves original distances as much as possible while focusing on small distances in the output space (conversely to <a href="/wiki/Sammon%27s_mapping" class="mw-redirect" title="Sammon&#39;s mapping">Sammon's mapping</a> which focus on small distances in original space).
</p><p>It should be noticed that CCA, as an iterative learning algorithm, actually starts with focus on large distances (like the Sammon algorithm), then gradually change focus to small distances. The small distance information will overwrite the large distance information, if compromises between the two have to be made.
</p><p>The stress function of CCA is related to a sum of right Bregman divergences<sup id="cite_ref-Jigang_29-0" class="reference"><a href="#cite_note-Jigang-29">&#91;29&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Curvilinear_distance_analysis">Curvilinear distance analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=15" title="Edit section: Curvilinear distance analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CDA<sup id="cite_ref-Demart_28-1" class="reference"><a href="#cite_note-Demart-28">&#91;28&#93;</a></sup> trains a self-organizing neural network to fit the manifold and seeks to preserve <a href="/wiki/Geodesic_distance" class="mw-redirect" title="Geodesic distance">geodesic distances</a> in its embedding. It is based on Curvilinear Component Analysis (which extended Sammon's mapping), but uses geodesic distances instead.
</p>
<h3><span class="mw-headline" id="Diffeomorphic_dimensionality_reduction">Diffeomorphic dimensionality reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=16" title="Edit section: Diffeomorphic dimensionality reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Diffeomorphic" class="mw-redirect" title="Diffeomorphic">Diffeomorphic</a> Dimensionality Reduction or <i>Diffeomap</i><sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup> learns a smooth diffeomorphic mapping which transports the data onto a lower-dimensional linear subspace. The methods solves for a smooth time indexed vector field such that flows along the field which start at the data points will end at a lower-dimensional linear subspace, thereby attempting to preserve pairwise differences under both the forward and inverse mapping.
</p>
<h3><span class="mw-headline" id="Kernel_principal_component_analysis">Kernel principal component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=17" title="Edit section: Kernel principal component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Perhaps the most widely used algorithm for manifold learning is <a href="/wiki/Kernel_principal_component_analysis" title="Kernel principal component analysis">kernel PCA</a>.<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup> It is a combination of <a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a> and the <a href="/wiki/Kernel_trick" class="mw-redirect" title="Kernel trick">kernel trick</a>. PCA begins by computing the covariance matrix of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle m\times n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>m</mi>
        <mo>&#x00D7;<!-- × --></mo>
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle m\times n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/12b23d207d23dd430b93320539abbb0bde84870d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:6.276ex; height:1.676ex;" alt="m\times n"/></span> matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {X} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">X</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {X} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f75966a2f9d5672136fa9401ee1e75008f95ffd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="\mathbf {X} "/></span>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\mathbf {x} _{i}\mathbf {x} _{i}^{\mathsf {T}}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>m</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <msubsup>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="sans-serif">T</mi>
              </mrow>
            </mrow>
          </msubsup>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\mathbf {x} _{i}\mathbf {x} _{i}^{\mathsf {T}}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3606df7c7cb77f57490c8ca9c9380b34cbd9a8f5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:17.49ex; height:6.843ex;" alt="C={\frac {1}{m}}\sum _{i=1}^{m}{\mathbf {x} _{i}\mathbf {x} _{i}^{\mathsf {T}}}."/></span></dd></dl>
<p>It then projects the data onto the first <i>k</i> eigenvectors of that matrix. By comparison, KPCA begins by computing the covariance matrix of the data after being transformed into a higher-dimensional space,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\Phi (\mathbf {x} _{i})\Phi (\mathbf {x} _{i})^{\mathsf {T}}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>m</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">&#x03A6;<!-- Φ --></mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
          <mi mathvariant="normal">&#x03A6;<!-- Φ --></mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <msup>
            <mo stretchy="false">)</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="sans-serif">T</mi>
              </mrow>
            </mrow>
          </msup>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\Phi (\mathbf {x} _{i})\Phi (\mathbf {x} _{i})^{\mathsf {T}}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ce442431fef9f2e1f868a550f94d1425da4503c9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:25.265ex; height:6.843ex;" alt="C={\frac {1}{m}}\sum _{i=1}^{m}{\Phi (\mathbf {x} _{i})\Phi (\mathbf {x} _{i})^{\mathsf {T}}}."/></span></dd></dl>
<p>It then projects the transformed data onto the first <i>k</i> eigenvectors of that matrix, just like PCA. It uses the kernel trick to factor away much of the computation, such that the entire process can be performed without actually computing <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Phi (\mathbf {x} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x03A6;<!-- Φ --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Phi (\mathbf {x} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/24456514d4d4c7d8e48a78f19906cd67c3655f63" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.898ex; height:2.843ex;" alt="\Phi (\mathbf {x} )"/></span>. Of course <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Phi }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x03A6;<!-- Φ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Phi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/aed80a2011a3912b028ba32a52dfa57165455f24" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.678ex; height:2.176ex;" alt="\Phi "/></span> must be chosen such that it has a known corresponding kernel. Unfortunately, it is not trivial to find a good kernel for a given problem, so KPCA does not yield good results with some problems when using standard kernels. For example, it is known to perform poorly with these kernels on the <a href="/wiki/Swiss_roll" title="Swiss roll">Swiss roll</a> manifold. However, one can view certain other methods that perform well in such settings (e.g., Laplacian Eigenmaps, LLE) as special cases of kernel PCA by constructing a data-dependent kernel matrix.<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup>
</p><p>KPCA has an internal model, so it can be used to map points onto its embedding that were not available at training time.
</p>
<h3><span class="mw-headline" id="Manifold_alignment">Manifold alignment</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=18" title="Edit section: Manifold alignment">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Manifold_alignment" title="Manifold alignment">Manifold alignment</a> takes advantage of the assumption that disparate data sets produced by similar generating processes will share a similar underlying manifold representation. By learning projections from each original space to the shared manifold, correspondences are recovered and knowledge from one domain can be transferred to another. Most manifold alignment techniques consider only two data sets, but the concept extends to arbitrarily many initial data sets.<sup id="cite_ref-33" class="reference"><a href="#cite_note-33">&#91;33&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Diffusion_maps">Diffusion maps</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=19" title="Edit section: Diffusion maps">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Diffusion_map" title="Diffusion map">Diffusion maps</a> leverages the relationship between heat <a href="/wiki/Diffusion" title="Diffusion">diffusion</a> and a <a href="/wiki/Random_walk" title="Random walk">random walk</a> (<a href="/wiki/Markov_Chain" class="mw-redirect" title="Markov Chain">Markov Chain</a>); an analogy is drawn between the diffusion operator on a manifold and a Markov transition matrix operating on functions defined on the graph whose nodes were sampled from the manifold.<sup id="cite_ref-34" class="reference"><a href="#cite_note-34">&#91;34&#93;</a></sup> In particular, let a data set be represented by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {X} =[x_{1},x_{2},\ldots ,x_{n}]\in \Omega \subset \mathbf {R^{D}} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">X</mi>
        </mrow>
        <mo>=</mo>
        <mo stretchy="false">[</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">]</mo>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi mathvariant="normal">&#x03A9;<!-- Ω --></mi>
        <mo>&#x2282;<!-- ⊂ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">R</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">D</mi>
            </mrow>
          </msup>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {X} =[x_{1},x_{2},\ldots ,x_{n}]\in \Omega \subset \mathbf {R^{D}} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/27767b00cc5714632b393ff3d9173e6eece42867" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:31.242ex; height:3.176ex;" alt="\mathbf {X} =[x_{1},x_{2},\ldots ,x_{n}]\in \Omega \subset \mathbf {R^{D}} "/></span>. The underlying assumption of diffusion map is that the high-dimensional data lies on a low-dimensional manifold of dimension <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {d} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">d</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {d} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8ec3b626fc045b6ff579316e29978fccfed884c2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;" alt="\mathbf {d} "/></span>. Let <b>X</b> represent the data set and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mu }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03BC;<!-- μ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mu }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9fd47b2a39f7a7856952afec1f1db72c67af6161" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.402ex; height:2.176ex;" alt="\mu "/></span> represent the distribution of the data points on <b>X</b>. Further, define a <b>kernel</b> which represents some notion of affinity of the points in <b>X</b>. The kernel <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathit {k}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-mathit" mathvariant="italic">k</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathit {k}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2acbed51a293a14971b3a890107329cc63bbc055" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; margin-right: -0.088ex; width:1.158ex; height:2.176ex;" alt="{\mathit {k}}"/></span> has the following properties<sup id="cite_ref-ReferenceA_35-0" class="reference"><a href="#cite_note-ReferenceA-35">&#91;35&#93;</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k(x,y)=k(y,x),\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>k</mi>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>,</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mspace width="thinmathspace" />
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k(x,y)=k(y,x),\,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1304950773f768dc626e3e7daa68caec312d0f9b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.212ex; height:2.843ex;" alt="k(x,y)=k(y,x),\,"/></span></dd></dl>
<p><i>k</i> is symmetric
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k(x,y)\geq 0\qquad \forall x,y,k}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
        <mo>&#x2265;<!-- ≥ --></mo>
        <mn>0</mn>
        <mspace width="2em" />
        <mi mathvariant="normal">&#x2200;<!-- ∀ --></mi>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo>,</mo>
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k(x,y)\geq 0\qquad \forall x,y,k}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e5cbd0e2f6010f21c4f2d7395496e5bb4a6133f2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:22.503ex; height:2.843ex;" alt="k(x,y)\geq 0\qquad \forall x,y,k"/></span></dd></dl>
<p><i>k</i> is positivity preserving
</p><p>Thus one can think of the individual data points as the nodes of a graph and the kernel <i>k</i> as defining some sort of affinity on that graph. The graph is symmetric by construction since the kernel is symmetric. It is easy to see here that from the tuple (<b>X</b>,<b>k</b>) one can construct a reversible <a href="/wiki/Markov_Chain" class="mw-redirect" title="Markov Chain">Markov Chain</a>. This technique is common to a variety of fields and is known as the graph Laplacian.
</p><p>For example, the graph <b>K</b> = (<i>X</i>,<i>E</i>) can be constructed using a Gaussian kernel.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K_{ij}={\begin{cases}e^{-\|x_{i}-x_{j}\|_{2}^{2}/\sigma ^{2}}&amp;{\text{if }}x_{i}\sim x_{j}\\0&amp;{\text{otherwise}}\end{cases}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>K</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>{</mo>
            <mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false">
              <mtr>
                <mtd>
                  <msup>
                    <mi>e</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mo>&#x2212;<!-- − --></mo>
                      <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
                      <msub>
                        <mi>x</mi>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi>i</mi>
                        </mrow>
                      </msub>
                      <mo>&#x2212;<!-- − --></mo>
                      <msub>
                        <mi>x</mi>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi>j</mi>
                        </mrow>
                      </msub>
                      <msubsup>
                        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mn>2</mn>
                        </mrow>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mn>2</mn>
                        </mrow>
                      </msubsup>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mo>/</mo>
                      </mrow>
                      <msup>
                        <mi>&#x03C3;<!-- σ --></mi>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mn>2</mn>
                        </mrow>
                      </msup>
                    </mrow>
                  </msup>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>if&#xA0;</mtext>
                  </mrow>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>i</mi>
                    </mrow>
                  </msub>
                  <mo>&#x223C;<!-- ∼ --></mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>otherwise</mtext>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo fence="true" stretchy="true" symmetric="true"></mo>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K_{ij}={\begin{cases}e^{-\|x_{i}-x_{j}\|_{2}^{2}/\sigma ^{2}}&amp;{\text{if }}x_{i}\sim x_{j}\\0&amp;{\text{otherwise}}\end{cases}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7cb0d2ac1637a4ea11c92a3d23d3c51e0f053f9b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:33.105ex; height:6.509ex;" alt="K_{ij}={\begin{cases}e^{-\|x_{i}-x_{j}\|_{2}^{2}/\sigma ^{2}}&amp;{\text{if }}x_{i}\sim x_{j}\\0&amp;{\text{otherwise}}\end{cases}}"/></span></dd></dl>
<p>In the above equation, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{i}\sim x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>&#x223C;<!-- ∼ --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}\sim x_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/683edd3f6501abdc531a4210ef6b67ab17d2f937" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.467ex; height:2.343ex;" alt="x_{i}\sim x_{j}"/></span> denotes that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;" alt="x_{i}"/></span> is a nearest neighbor of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5db47cb3d2f9496205a17a6856c91c1d3d363ccd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.239ex; height:2.343ex;" alt="x_{j}"/></span>. Properly, <a href="/wiki/Geodesic" title="Geodesic">Geodesic</a> distance should be used to actually measure distances on the <a href="/wiki/Manifold" title="Manifold">manifold</a>. Since the exact structure of the manifold is not available, for the nearest neighbors the geodesic distance is approximated by euclidean distance. The choice <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C3;<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sigma }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/59f59b7c3e6fdb1d0365a494b81fb9a696138c36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="\sigma "/></span> modulates our notion of proximity in the sense that if <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \|x_{i}-x_{j}\|_{2}\gg \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>&#x226B;<!-- ≫ --></mo>
        <mi>&#x03C3;<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|x_{i}-x_{j}\|_{2}\gg \sigma }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7c94b612bf2705bc301b280d336dc7c2f0d0c117" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:15.532ex; height:3.009ex;" alt="\|x_{i}-x_{j}\|_{2}\gg \sigma "/></span> then <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K_{ij}=0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>K</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K_{ij}=0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3cce5f477da31b516c64aa6cdb7911989271b644" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.711ex; height:2.843ex;" alt="K_{ij}=0"/></span> and if <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \|x_{i}-x_{j}\|_{2}\ll \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>&#x226A;<!-- ≪ --></mo>
        <mi>&#x03C3;<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|x_{i}-x_{j}\|_{2}\ll \sigma }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/17a6fb63a900ed8e4b08ab33994f4e5fac9a77ef" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:15.532ex; height:3.009ex;" alt="\|x_{i}-x_{j}\|_{2}\ll \sigma "/></span> then <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K_{ij}=1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>K</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K_{ij}=1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/220951965b0d01826f686ac18d2bf566455cea72" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.711ex; height:2.843ex;" alt="K_{ij}=1"/></span>. The former means that very little diffusion has taken place while the latter implies that the diffusion process is nearly complete. Different strategies to choose <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C3;<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sigma }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/59f59b7c3e6fdb1d0365a494b81fb9a696138c36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="\sigma "/></span> can be found in.<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup>
</p><p>In order to faithfully represent a Markov matrix, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;" alt="K"/></span> must be normalized by the corresponding <a href="/wiki/Degree_matrix" title="Degree matrix">degree matrix</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>D</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.924ex; height:2.176ex;" alt="D"/></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P=D^{-1}K.\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo>=</mo>
        <msup>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mi>K</mi>
        <mo>.</mo>
        <mspace width="thinmathspace" />
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P=D^{-1}K.\,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eb029075ae4592f7e78f75c0bee5d9ecbcfaf17e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:12.201ex; height:2.676ex;" alt="P=D^{-1}K.\,"/></span></dd></dl>
<p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b4dc73bf40314945ff376bd363916a738548d40a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.745ex; height:2.176ex;" alt="P"/></span> now represents a Markov chain. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(x_{i},x_{j})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(x_{i},x_{j})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ed08139d79724fd6a470d8d02bd3f92c95e98b4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:8.957ex; height:3.009ex;" alt="P(x_{i},x_{j})"/></span> is the probability of transitioning from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;" alt="x_{i}"/></span> to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5db47cb3d2f9496205a17a6856c91c1d3d363ccd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.239ex; height:2.343ex;" alt="x_{j}"/></span> in one time step. Similarly the probability of transitioning from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;" alt="x_{i}"/></span> to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5db47cb3d2f9496205a17a6856c91c1d3d363ccd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.239ex; height:2.343ex;" alt="x_{j}"/></span> in <b>t</b> time steps is given by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P^{t}(x_{i},x_{j})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P^{t}(x_{i},x_{j})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/71719d0f7f63c9128f27ce0e0d31d9d0baedde56" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:9.859ex; height:3.176ex;" alt="P^{t}(x_{i},x_{j})"/></span>. Here <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P^{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P^{t}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3874f8df55758c9c93fc87e7d3540f24ab666faf" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.647ex; height:2.509ex;" alt="P^{t}"/></span> is the matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b4dc73bf40314945ff376bd363916a738548d40a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.745ex; height:2.176ex;" alt="P"/></span> multiplied by itself <b>t</b> times.
</p><p>The Markov matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b4dc73bf40314945ff376bd363916a738548d40a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.745ex; height:2.176ex;" alt="P"/></span> constitutes some notion of local geometry of the data set <b>X</b>. The major difference between diffusion maps and <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> is that only local features of the data are considered in diffusion maps as opposed to taking correlations of the entire data set.
</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;" alt="K"/></span> defines a random walk on the data set which means that the kernel captures some local geometry of data set. The Markov chain defines fast and slow directions of propagation through the kernel values. As the walk propagates forward in time, the local geometry information aggregates in the same way as local transitions (defined by differential equations) of the dynamical system.<sup id="cite_ref-ReferenceA_35-1" class="reference"><a href="#cite_note-ReferenceA-35">&#91;35&#93;</a></sup> The metaphor of diffusion arises from the definition of a family diffusion distance  {<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/875ab92f7e53970140b3663bc81e5fdcd9528a63" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"/></span>}<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle _{t\in N}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>N</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle _{t\in N}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/122e751d6f6ce6cbce64f4f0838a379c315cb0c9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.381ex; height:1.676ex;" alt="_{t\in N}"/></span>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D_{t}^{2}(x,y)=||p_{t}(x,\cdot )-p_{t}(y,\cdot )||^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msubsup>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">)</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>,</mo>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">|</mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}^{2}(x,y)=||p_{t}(x,\cdot )-p_{t}(y,\cdot )||^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a3d5eae037d1c6269e9b9319586317597405528" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:31.343ex; height:3.509ex;" alt="D_{t}^{2}(x,y)=||p_{t}(x,\cdot )-p_{t}(y,\cdot )||^{2}"/></span></dd></dl>
<p>For fixed t, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/875ab92f7e53970140b3663bc81e5fdcd9528a63" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"/></span> defines a distance between any two points of the data set based on path connectivity: the value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D_{t}(x,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}(x,y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/29152128bcc76b04fc7e91db93358e8c6b31c2df" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.079ex; height:2.843ex;" alt="D_{t}(x,y)"/></span> will be smaller the more paths that connect <b>x</b> to <b>y</b> and vice versa. Because the quantity <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D_{t}(x,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}(x,y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/29152128bcc76b04fc7e91db93358e8c6b31c2df" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.079ex; height:2.843ex;" alt="D_{t}(x,y)"/></span> involves a sum over of all paths of length t, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/875ab92f7e53970140b3663bc81e5fdcd9528a63" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"/></span> is much more robust to noise in the data than geodesic distance. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/875ab92f7e53970140b3663bc81e5fdcd9528a63" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"/></span> takes into account all the relation between points x and y while calculating the distance and serves as a better notion of proximity than just <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> or even geodesic distance.
</p>
<h3><span id="Hessian_Locally-Linear_Embedding_.28Hessian_LLE.29"></span><span class="mw-headline" id="Hessian_Locally-Linear_Embedding_(Hessian_LLE)">Hessian Locally-Linear Embedding (Hessian LLE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=20" title="Edit section: Hessian Locally-Linear Embedding (Hessian LLE)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Like LLE, <a href="/w/index.php?title=Hessian_LLE&amp;action=edit&amp;redlink=1" class="new" title="Hessian LLE (page does not exist)">Hessian LLE</a><sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup> is also based on sparse matrix techniques. It tends to yield results of a much higher quality than LLE. Unfortunately, it has a very costly computational complexity, so it is not well-suited for heavily sampled manifolds. It has no internal model.
</p>
<h3><span id="Modified_Locally-Linear_Embedding_.28MLLE.29"></span><span class="mw-headline" id="Modified_Locally-Linear_Embedding_(MLLE)">Modified Locally-Linear Embedding (MLLE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=21" title="Edit section: Modified Locally-Linear Embedding (MLLE)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Modified LLE (MLLE)<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup> is another LLE variant which uses multiple weights in each neighborhood to address the local weight matrix conditioning problem which leads to distortions in LLE maps.  MLLE produces robust projections similar to Hessian LLE, but without the significant additional computational cost.
</p>
<h3><span class="mw-headline" id="Relational_perspective_map">Relational perspective map</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=22" title="Edit section: Relational perspective map">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Relational perspective map is a <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a> algorithm. The algorithm finds a configuration of data points on a manifold by simulating a multi-particle dynamic system on a closed manifold, where data points are mapped to particles and distances (or dissimilarity) between data points represent a repulsive force. As the manifold gradually grows in size the multi-particle system cools down gradually and converges to a configuration that reflects the distance information of the data points.
</p><p>Relational perspective map was inspired by a physical model in which positively charged particles move freely on the surface of a ball.  Guided by the <a href="/wiki/Charles-Augustin_de_Coulomb" title="Charles-Augustin de Coulomb">Coulomb</a> <a href="/wiki/Coulomb%27s_law" title="Coulomb&#39;s law">force</a> between particles, the minimal energy configuration of the particles will reflect the strength of repulsive forces between the particles.
</p><p>The Relational perspective map was introduced in.<sup id="cite_ref-39" class="reference"><a href="#cite_note-39">&#91;39&#93;</a></sup>
The algorithm firstly used the flat <a href="/wiki/Torus" title="Torus">torus</a> as the image manifold, then it has been extended (in the software <a rel="nofollow" class="external text" href="http://www.VisuMap.com">VisuMap</a> to use other types of closed manifolds, like the <a href="/wiki/Sphere" title="Sphere">sphere</a>, <a href="/wiki/Projective_space" title="Projective space">projective space</a>, and <a href="/wiki/Klein_bottle" title="Klein bottle">Klein bottle</a>, as image manifolds.
</p>
<h3><span class="mw-headline" id="Local_tangent_space_alignment">Local tangent space alignment</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=23" title="Edit section: Local tangent space alignment">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Local_tangent_space_alignment" title="Local tangent space alignment">Local tangent space alignment</a></div>
<p><a href="/wiki/Local_tangent_space_alignment" title="Local tangent space alignment">LTSA</a><sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup> is based on the intuition that when a manifold is correctly unfolded, all of the tangent hyperplanes to the manifold will become aligned.  It begins by computing the <i>k</i>-nearest neighbors of every point.  It computes the tangent space at every point by computing the <i>d</i>-first principal components in each local neighborhood.  It then optimizes to find an embedding that aligns the tangent spaces.
</p>
<h3><span class="mw-headline" id="Local_multidimensional_scaling">Local multidimensional scaling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=24" title="Edit section: Local multidimensional scaling">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Local Multidimensional Scaling<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">&#91;41&#93;</a></sup> performs <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a> in local regions, and then uses convex optimization to fit all the pieces together.
</p>
<h3><span class="mw-headline" id="Maximum_variance_unfolding">Maximum variance unfolding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=25" title="Edit section: Maximum variance unfolding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Maximum_Variance_Unfolding" class="mw-redirect" title="Maximum Variance Unfolding">Maximum Variance Unfolding</a> was formerly known as Semidefinite Embedding. The intuition for this algorithm is that when a manifold is properly unfolded, the variance over the points is maximized. This algorithm also begins by finding the <i>k</i>-nearest neighbors of every point. It then seeks to solve the problem of maximizing the distance between all non-neighboring points, constrained such that the distances between neighboring points are preserved. The primary contribution of this algorithm is a technique for casting this problem as a semidefinite programming problem. Unfortunately, semidefinite programming solvers have a high computational cost. The Landmark–MVU variant of this algorithm uses landmarks to increase speed with some cost to accuracy. It has no model.
</p>
<h3><span class="mw-headline" id="Nonlinear_PCA">Nonlinear PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=26" title="Edit section: Nonlinear PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Nonlinear PCA<sup id="cite_ref-42" class="reference"><a href="#cite_note-42">&#91;42&#93;</a></sup> (NLPCA) uses <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> to train a multi-layer perceptron (MLP) to fit to a manifold. Unlike typical MLP training, which only updates the weights, NLPCA updates both the weights and the inputs. That is, both the weights and inputs are treated as latent values. After training, the latent inputs are a low-dimensional representation of the observed vectors, and the MLP maps from that low-dimensional representation to the high-dimensional observation space.
</p>
<h3><span class="mw-headline" id="Data-driven_high-dimensional_scaling">Data-driven high-dimensional scaling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=27" title="Edit section: Data-driven high-dimensional scaling">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Data-Driven High Dimensional Scaling (DD-HDS)<sup id="cite_ref-43" class="reference"><a href="#cite_note-43">&#91;43&#93;</a></sup> is closely related to <a href="/wiki/Sammon%27s_mapping" class="mw-redirect" title="Sammon&#39;s mapping">Sammon's mapping</a> and curvilinear component analysis except that (1) it simultaneously penalizes false neighborhoods and tears by focusing on small distances in both original and output space, and that (2) it accounts for <a href="/wiki/Concentration_of_measure" title="Concentration of measure">concentration of measure</a> phenomenon by adapting the weighting function to the distance distribution.
</p>
<h3><span class="mw-headline" id="Manifold_sculpting">Manifold sculpting</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=28" title="Edit section: Manifold sculpting">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Manifold Sculpting<sup id="cite_ref-44" class="reference"><a href="#cite_note-44">&#91;44&#93;</a></sup> uses <a href="/wiki/Graduated_optimization" title="Graduated optimization">graduated optimization</a> to find an embedding. Like other algorithms, it computes the <i>k</i>-nearest neighbors and tries to seek an embedding that preserves relationships in local neighborhoods. It slowly scales variance out of higher dimensions, while simultaneously adjusting points in lower dimensions to preserve those relationships. If the rate of scaling is small, it can find very precise embeddings. It boasts higher empirical accuracy than other algorithms with several problems. It can also be used to refine the results from other manifold learning algorithms. It struggles to unfold some manifolds, however, unless a very slow scaling rate is used. It has no model.
</p>
<h3><span class="mw-headline" id="t-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embedding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=29" title="Edit section: t-distributed stochastic neighbor embedding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-distributed stochastic neighbor embedding</a> (t-SNE) <sup id="cite_ref-45" class="reference"><a href="#cite_note-45">&#91;45&#93;</a></sup> is widely used. It is one of a family of stochastic neighbor embedding methods. The algorithm computes the probability that pairs of datapoints in the high-dimensional space are related, and then chooses low-dimensional embeddings which produce a similar distribution.
</p>
<h3><span class="mw-headline" id="RankVisu">RankVisu</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=30" title="Edit section: RankVisu">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>RankVisu<sup id="cite_ref-46" class="reference"><a href="#cite_note-46">&#91;46&#93;</a></sup> is designed to preserve rank of neighborhood rather than distance. RankVisu is especially useful on difficult tasks (when the preservation of distance cannot be achieved satisfyingly). Indeed, the rank of neighborhood is less informative than distance (ranks can be deduced from distances but distances cannot be deduced from ranks) and its preservation is thus easier.
</p>
<h3><span class="mw-headline" id="Topologically_constrained_isometric_embedding">Topologically constrained isometric embedding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=31" title="Edit section: Topologically constrained isometric embedding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/w/index.php?title=Topologically_Constrained_Isometric_Embedding&amp;action=edit&amp;redlink=1" class="new" title="Topologically Constrained Isometric Embedding (page does not exist)">Topologically Constrained Isometric Embedding</a> (TCIE)<sup id="cite_ref-47" class="reference"><a href="#cite_note-47">&#91;47&#93;</a></sup> is an algorithm based approximating geodesic distances after filtering geodesics inconsistent with the Euclidean metric. Aimed at correcting the distortions caused when Isomap is used to map intrinsically non-convex data, TCIE uses weight least-squares MDS in order to obtain a more accurate mapping. The TCIE algorithm first detects possible boundary points in the data, and during computation of the geodesic length marks inconsistent geodesics, to be given a small weight in the weighted <a href="/wiki/Stress_majorization" title="Stress majorization">Stress majorization</a> that follows.
</p>
<h2><span class="mw-headline" id="Methods_based_on_proximity_matrices">Methods based on proximity matrices</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=32" title="Edit section: Methods based on proximity matrices">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A method based on proximity matrices is one where the data is presented to the algorithm in the form of a <a href="/wiki/Similarity_matrix" class="mw-redirect" title="Similarity matrix">similarity matrix</a> or a <a href="/wiki/Distance_matrix" title="Distance matrix">distance matrix</a>. These methods all fall under the broader class of <a href="/wiki/Multidimensional_scaling#Types" title="Multidimensional scaling">metric multidimensional scaling</a>. The variations tend to be differences in how the proximity data is computed; for example, <a href="/wiki/Isomap" title="Isomap">Isomap</a>, <a href="/wiki/Locally_linear_embeddings" class="mw-redirect" title="Locally linear embeddings">locally linear embeddings</a>, <a href="/wiki/Maximum_variance_unfolding" class="mw-redirect" title="Maximum variance unfolding">maximum variance unfolding</a>, and <a href="/wiki/Sammon%27s_projection" class="mw-redirect" title="Sammon&#39;s projection">Sammon mapping</a> (which is not in fact a mapping) are examples of metric multidimensional scaling methods.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=33" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Discriminant_analysis" class="mw-redirect" title="Discriminant analysis">Discriminant analysis</a></li>
<li><a href="/wiki/Elastic_map" title="Elastic map">Elastic map</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Growing_self-organizing_map" title="Growing self-organizing map">Growing self-organizing map</a> (GSOM)</li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">Self-organizing map</a> (SOM)</li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=34" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lawrence, Neil D (2012). <a rel="nofollow" class="external text" href="http://www.jmlr.org/papers/v13/lawrence12a.html">"A unifying probabilistic perspective for spectral dimensionality reduction: insights and new models"</a>. <i><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">Journal of Machine Learning Research</a></i>. <b>13</b> (May): 1609–1638.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=A+unifying+probabilistic+perspective+for+spectral+dimensionality+reduction%3A+insights+and+new+models&amp;rft.volume=13&amp;rft.issue=May&amp;rft.pages=1609-1638&amp;rft.date=2012&amp;rft.aulast=Lawrence&amp;rft.aufirst=Neil+D&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fv13%2Flawrence12a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">John A. Lee, Michel Verleysen, Nonlinear Dimensionality Reduction, Springer, 2007.</span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">Gashler, M. and Martinez, T., <a rel="nofollow" class="external text" href="http://axon.cs.byu.edu/papers/gashler2011ijcnn2.pdf">Temporal Nonlinear Dimensionality Reduction</a>, In <i>Proceedings of the International Joint Conference on Neural Networks IJCNN'11</i>, pp. 1959–1966, 2011</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">J. B. Tenenbaum, V. de Silva, J. C. Langford, A Global Geometric Framework for Nonlinear Dimensionality Reduction, Science 290, (2000), 2319&#x2013;2323.</span>
</li>
<li id="cite_note-:0-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_5-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Vepakomma, Praneeth; Elgammal, Ahmed (May 2016). "A fast algorithm for manifold learning by posing it as a symmetric diagonally dominant linear system". <i>Applied and Computational Harmonic Analysis</i>. <b>40</b> (3): 622–628. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.acha.2015.10.004">10.1016/j.acha.2015.10.004</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1063-5203">1063-5203</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Applied+and+Computational+Harmonic+Analysis&amp;rft.atitle=A+fast+algorithm+for+manifold+learning+by+posing+it+as+a+symmetric+diagonally+dominant+linear+system&amp;rft.volume=40&amp;rft.issue=3&amp;rft.pages=622-628&amp;rft.date=2016-05&amp;rft_id=info%3Adoi%2F10.1016%2Fj.acha.2015.10.004&amp;rft.issn=1063-5203&amp;rft.aulast=Vepakomma&amp;rft.aufirst=Praneeth&amp;rft.au=Elgammal%2C+Ahmed&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">S. T. Roweis and L. K. Saul, Nonlinear Dimensionality Reduction by Locally Linear Embedding, Science Vol 290, 22 December 2000, 2323&#x2013;2326.</span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text">Mikhail Belkin and <a href="/wiki/Partha_Niyogi" title="Partha Niyogi">Partha Niyogi</a>, Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering, Advances in Neural Information Processing Systems 14, 2001, p. 586–691, MIT Press</span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Mikhail Belkin Problems of Learning on Manifolds, PhD Thesis, Department of Mathematics, The University Of Chicago, August 2003</span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Bengio et al. "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering" in Advances in Neural Information Processing Systems (2004)</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Mikhail Belkin Problems of Learning on Manifolds, PhD Thesis, Department of Mathematics, The <a href="/wiki/University_Of_Chicago" class="mw-redirect" title="University Of Chicago">University Of Chicago</a>, August 2003</span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.cse.ohio-state.edu/~mbelkin/algorithms/algorithms.html">Ohio-state.edu</a></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.cse.ohio-state.edu/~mbelkin/papers/papers.html#thesis">Ohio-state.edu</a></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text">J. Tenenbaum and W. Freeman, Separating style and content with bilinear models, Neural Computation, vol. 12, 2000.</span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text">M. Lewandowski, J. Martinez-del Rincon, D. Makris, and J.-C. Nebel, Temporal extension of laplacian eigenmaps for unsupervised dimensionality reduction of time series, Proceedings of the International Conference on Pattern Recognition (ICPR), 2010</span>
</li>
<li id="cite_note-ReferenceB-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-ReferenceB_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ReferenceB_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">M. Lewandowski, D. Makris, S.A. Velastin and J.-C. Nebel, Structural Laplacian Eigenmaps for Modeling Sets of Multivariate Sequences, IEEE Transactions on Cybernetics, 44(6): 936-949, 2014</span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">J. Martinez-del-Rincon, M. Lewandowski, J.-C. Nebel and D. Makris, Generalized Laplacian Eigenmaps for Modeling and Tracking Human Motions, IEEE Transactions on Cybernetics, 44(9), pp 1646-1660, 2014</span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text">The illustration is prepared using free software: E.M. Mirkes, <a rel="nofollow" class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/PCA_SOM/PCA_SOM.html">Principal Component Analysis and Self-Organizing Maps: applet</a>. University of Leicester, 2011</span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Yin, Hujun; <a rel="nofollow" class="external text" href="http://pca.narod.ru/contentsgkwz.htm"><i>Learning Nonlinear Principal Manifolds by Self-Organising Maps</i></a>, in A.N. Gorban, B. Kégl, D.C. Wunsch, and A. Zinovyev (Eds.), <i>Principal Manifolds for Data Visualization and Dimension Reduction</i>, Lecture Notes in Computer Science and Engineering (LNCSE), vol. 58, Berlin, Germany: Springer, 2007, Ch. 3, pp. 68-95. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-540-73749-0" title="Special:BookSources/978-3-540-73749-0">978-3-540-73749-0</a></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">A. N. Gorban, A. Zinovyev, <a rel="nofollow" class="external text" href="https://arxiv.org/abs/1001.1122">Principal manifolds and graphs in practice: from molecular biology to dynamical systems</a>, <a href="/wiki/International_Journal_of_Neural_Systems" title="International Journal of Neural Systems">International Journal of Neural Systems</a>, Vol. 20, No. 3 (2010) 219–232.</span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">A. Zinovyev, <a rel="nofollow" class="external text" href="http://bioinfo-out.curie.fr/projects/vidaexpert/">ViDaExpert</a> - Multidimensional Data Visualization Tool (free for non-commercial use). <a href="/wiki/Curie_Institute_(Paris)" title="Curie Institute (Paris)">Institut Curie</a>, Paris.</span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">A. Zinovyev, <a rel="nofollow" class="external text" href="http://www.ihes.fr/~zinovyev/vida/ViDaExpert/ViDaOverView.pdf">ViDaExpert overview</a>, <a rel="nofollow" class="external text" href="http://www.ihes.fr">IHES</a> (<a href="/wiki/Institut_des_Hautes_%C3%89tudes_Scientifiques" title="Institut des Hautes Études Scientifiques">Institut des Hautes Études Scientifiques</a>), Bures-Sur-Yvette, Île-de-France.</span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text">T. Hastie, Principal Curves and Surfaces, Ph.D Dissertation, Stanford Linear Accelerator Center, Stanford University, Stanford, California, US, November 1984.</span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><a href="/wiki/Alexander_Nikolaevich_Gorban" class="mw-redirect" title="Alexander Nikolaevich Gorban">A.N. Gorban</a>, B. Kégl, D.C. Wunsch, A. Zinovyev  (Eds.), <a rel="nofollow" class="external text" href="https://www.researchgate.net/publication/271642170_Principal_Manifolds_for_Data_Visualisation_and_Dimension_Reduction_LNCSE_58">Principal Manifolds for Data Visualisation and Dimension Reduction</a>, Lecture Notes in Computer Science and Engineering (LNCSE), Vol. 58, Springer, Berlin &#8211; Heidelberg &#8211; New York, 2007. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-540-73749-0" title="Special:BookSources/978-3-540-73749-0">978-3-540-73749-0</a></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (October 2006). "Nonlocal Estimation of Manifold Structure". <i>Neural Computation</i>. <b>18</b> (10): 2509–2528. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1162%2Fneco.2006.18.10.2509">10.1162/neco.2006.18.10.2509</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0899-7667">0899-7667</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16907635">16907635</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Nonlocal+Estimation+of+Manifold+Structure&amp;rft.volume=18&amp;rft.issue=10&amp;rft.pages=2509-2528&amp;rft.date=2006-10&amp;rft.issn=0899-7667&amp;rft_id=info%3Apmid%2F16907635&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.10.2509&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=Monperrus%2C+Martin&amp;rft.au=Larochelle%2C+Hugo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">N. Lawrence, <a rel="nofollow" class="external text" href="http://jmlr.csail.mit.edu/papers/v6/lawrence05a.html">Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a>, Journal of Machine Learning Research 6(Nov):1783–1816, 2005.</span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text">M. Ding, G. Fan, <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/abstract/document/6985586/">Multilayer Joint Gait-Pose Manifolds for Human Gait Motion Modeling</a>, IEEE Transactions on Cybernetics, Volume: 45, Issue: 11, Nov 2015.</span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text">Taylor, D., Klimm, F., Harrington, H. A., Kramár, M., Mischaikow, K., Porter, M. A., &amp; Mucha, P. J. (2015). Topological data analysis of contagion maps for examining spreading processes on networks. Nature Communications, 6, 7723.</span>
</li>
<li id="cite_note-Demart-28"><span class="mw-cite-backlink">^ <a href="#cite_ref-Demart_28-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Demart_28-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">P. Demartines and J. Hérault, Curvilinear Component Analysis: A Self-Organizing Neural Network for Nonlinear Mapping of Data Sets, IEEE Transactions on Neural Networks, Vol. 8(1), 1997, pp. 148–154</span>
</li>
<li id="cite_note-Jigang-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-Jigang_29-0">^</a></b></span> <span class="reference-text">Jigang Sun, Malcolm Crowe, and Colin Fyfe,  <a rel="nofollow" class="external text" href="http://www.dice.ucl.ac.be/Proceedings/esann/esannpdf/es2010-107.pdf">Curvilinear component analysis and Bregman divergences</a>, In European Symposium on Artificial Neural Networks (Esann), pages 81–86. d-side publications, 2010</span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text">Christian Walder and Bernhard Schölkopf, Diffeomorphic Dimensionality Reduction, Advances in Neural Information Processing Systems 22, 2009, pp. 1713–1720, MIT Press</span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text">B. Schölkopf, A. Smola, K.-R. Müller, Nonlinear Component Analysis as a Kernel Eigenvalue Problem. <i>Neural Computation </i>10(5):1299-1319, 1998, <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a> Cambridge, MA, USA, <a href="//doi.org/10.1162/089976698300017467" class="extiw" title="doi:10.1162/089976698300017467">doi:10.1162/089976698300017467</a></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text">Jihun Ham, Daniel D. Lee, Sebastian Mika, Bernhard Schölkopf. A kernel view of the dimensionality reduction of manifolds. Proceedings of the 21st International Conference on Machine Learning, Banff, Canada, 2004. <a href="//doi.org/10.1145/1015330.1015417" class="extiw" title="doi:10.1145/1015330.1015417">doi:10.1145/1015330.1015417</a></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation conference">Wang, Chang; Mahadevan, Sridhar (July 2008). <a rel="nofollow" class="external text" href="http://people.cs.umass.edu/~chwang/papers/ICML-2008.pdf"><i>Manifold Alignment using Procrustes Analysis</i></a> <span class="cs1-format">(PDF)</span>. The 25th International Conference on Machine Learning. pp.&#160;1120–1127.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Manifold+Alignment+using+Procrustes+Analysis&amp;rft.pages=1120-1127&amp;rft.date=2008-07&amp;rft.aulast=Wang&amp;rft.aufirst=Chang&amp;rft.au=Mahadevan%2C+Sridhar&amp;rft_id=http%3A%2F%2Fpeople.cs.umass.edu%2F~chwang%2Fpapers%2FICML-2008.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text">Diffusion Maps and Geometric Harmonics, Stephane Lafon, PhD Thesis, <a href="/wiki/Yale_University" title="Yale University">Yale University</a>, May 2004</span>
</li>
<li id="cite_note-ReferenceA-35"><span class="mw-cite-backlink">^ <a href="#cite_ref-ReferenceA_35-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ReferenceA_35-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Diffusion Maps, Ronald R. Coifman and Stephane Lafon,: Science, 19 June 2006</span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text">B. Bah, "Diffusion Maps: Applications and Analysis", Masters Thesis, University of Oxford</span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text">D. Donoho and C. Grimes, "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data" Proc Natl Acad Sci U S A. 2003 May 13; 100(10): 5591–5596</span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text">Z. Zhang and J. Wang, "MLLE: Modified Locally Linear Embedding Using Multiple Weights" <a rel="nofollow" class="external free" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text">James X. Li, <a rel="nofollow" class="external text" href="http://www.palgrave-journals.com/ivs/journal/v3/n1/pdf/9500051a.pdf">Visualizing high-dimensional data with relational perspective map</a>, Information Visualization (2004) 3, 49–59</span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><cite class="citation journal">Zhang, Zhenyue; Hongyuan Zha (2005). "Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment". <i>SIAM Journal on Scientific Computing</i>. <b>26</b> (1): 313–338. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.211.9957">10.1.1.211.9957</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1137%2Fs1064827502419154">10.1137/s1064827502419154</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Journal+on+Scientific+Computing&amp;rft.atitle=Principal+Manifolds+and+Nonlinear+Dimension+Reduction+via+Local+Tangent+Space+Alignment&amp;rft.volume=26&amp;rft.issue=1&amp;rft.pages=313-338&amp;rft.date=2005&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.211.9957&amp;rft_id=info%3Adoi%2F10.1137%2Fs1064827502419154&amp;rft.aulast=Zhang&amp;rft.aufirst=Zhenyue&amp;rft.au=Hongyuan+Zha&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text">J Venna and S Kaski, Local multidimensional scaling, Neural Networks, 2006</span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text">Scholz, M. Kaplan, F. Guy, C. L. Kopka, J. Selbig, J., Non-linear PCA: a missing data approach, In <i>Bioinformatics</i>, Vol. 21, Number 20, pp. 3887–3895, Oxford University Press, 2005</span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text">S. Lespinats, M. Verleysen, A. Giron, B. Fertil, DD-HDS: a tool for visualization and exploration of high-dimensional data, IEEE Transactions on Neural Networks 18 (5) (2007) 1265–1279.</span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text">Gashler, M. and Ventura, D. and Martinez, T., <i><a rel="nofollow" class="external text" href="http://axon.cs.byu.edu/papers/gashler2007nips.pdf">Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></i>, In Platt, J.C. and Koller, D. and Singer, Y. and Roweis, S., editor, Advances in Neural Information Processing Systems 20, pp. 513–520, MIT Press, Cambridge, MA, 2008</span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><cite class="citation journal">van der Maaten, L.J.P.; Hinton, G.E. (Nov 2008). <a rel="nofollow" class="external text" href="http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">"Visualizing High-Dimensional Data Using t-SNE"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>9</b>: 2579–2605.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Visualizing+High-Dimensional+Data+Using+t-SNE&amp;rft.volume=9&amp;rft.pages=2579-2605&amp;rft.date=2008-11&amp;rft.aulast=van+der+Maaten&amp;rft.aufirst=L.J.P.&amp;rft.au=Hinton%2C+G.E.&amp;rft_id=http%3A%2F%2Fjmlr.org%2Fpapers%2Fvolume9%2Fvandermaaten08a%2Fvandermaaten08a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text">Lespinats S., Fertil B., Villemain P. and Herault J., Rankvisu: Mapping from the neighbourhood network, Neurocomputing, vol. 72 (13–15), pp. 2964–2978, 2009.</span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text">Rosman G., Bronstein M. M., Bronstein A. M. and Kimmel R., Nonlinear Dimensionality Reduction by Topologically Constrained Isometric Embedding, International Journal of Computer Vision, Volume 89, Number 1, 56–68, 2010</span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=35" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20040411051530/http://isomap.stanford.edu/">Isomap</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20090204021348/http://www.ncrg.aston.ac.uk/GTM/">Generative Topographic Mapping</a></li>
<li><a rel="nofollow" class="external text" href="http://www.miketipping.com/thesis.htm">Mike Tipping's Thesis</a></li>
<li><a rel="nofollow" class="external text" href="http://www.dcs.shef.ac.uk/~neil/gplvm/">Gaussian Process Latent Variable Model</a></li>
<li><a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~roweis/lle/">Locally Linear Embedding</a></li>
<li><a rel="nofollow" class="external text" href="http://www.visumap.net/index.aspx?p=Resources/RpmOverview">Relational Perspective Map</a></li>
<li><a rel="nofollow" class="external text" href="http://waffles.sourceforge.net/">Waffles</a> is an open source C++ library containing implementations of LLE, Manifold Sculpting, and some other manifold learning algorithms.</li>
<li><a rel="nofollow" class="external text" href="http://sy.lespi.free.fr/DD-HDS-homepage.html">DD-HDS homepage</a></li>
<li><a rel="nofollow" class="external text" href="http://sy.lespi.free.fr/RankVisu-homepage.html">RankVisu homepage</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20111002023651/http://tx.technion.ac.il/~rc/diffusion_maps.pdf">Short review of Diffusion Maps</a></li>
<li><a rel="nofollow" class="external text" href="http://www.nlpca.org/">Nonlinear PCA by autoencoder neural networks</a></li></ul>

<!-- 
NewPP limit report
Parsed by mw1243
Cached time: 20190304012407
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.552 seconds
Real time usage: 0.860 seconds
Preprocessor visited node count: 1618/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 15160/2097152 bytes
Template argument size: 1194/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 43569/5000000 bytes
Number of Wikibase entities loaded: 3/400
Lua time usage: 0.173/10.000 seconds
Lua memory usage: 3.49 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  409.433      1 -total
 71.07%  290.990      1 Template:Reflist
 42.09%  172.314      5 Template:Cite_journal
 15.14%   61.985      2 Template:ISBN
 12.11%   49.575      1 Template:Short_description
 11.28%   46.174      1 Template:Pagetype
  5.91%   24.201      2 Template:Catalog_lookup_link
  3.75%   15.357      2 Template:Error-small
  3.75%   15.347      1 Template:See_also
  3.01%   12.327      2 Template:Small
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:309261-0!canonical!math=5 and timestamp 20190304012407 and revision id 886008276
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;oldid=886008276#Manifold_learning_algorithms">https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;oldid=886008276#Manifold_learning_algorithms</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Nonlinear+dimensionality+reduction" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Nonlinear+dimensionality+reduction" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Nonlinear_dimensionality_reduction" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Nonlinear_dimensionality_reduction" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<ul class="menu">
													</ul>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Nonlinear_dimensionality_reduction">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<ul class="menu">
													</ul>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Nonlinear_dimensionality_reduction" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Nonlinear_dimensionality_reduction" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;oldid=886008276" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q7049464" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Nonlinear_dimensionality_reduction&amp;id=886008276" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Nonlinear+dimensionality+reduction">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Nonlinear+dimensionality+reduction&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Nonlinear_dimensionality_reduction&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%DA%A9%D8%A7%D9%87%D8%B4_%D8%BA%DB%8C%D8%B1%D8%AE%D8%B7%DB%8C_%D8%A7%D8%A8%D8%B9%D8%A7%D8%AF" title="کاهش غیرخطی ابعاد – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">فارسی</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4" title="非线性降维 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q7049464#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 3 March 2019, at 19:10<span class="anonymous-show">&#160;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;mobileaction=toggle_view_mobile#Manifold_learning_algorithms" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.552","walltime":"0.860","ppvisitednodes":{"value":1618,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":15160,"limit":2097152},"templateargumentsize":{"value":1194,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":3,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":43569,"limit":5000000},"entityaccesscount":{"value":3,"limit":400},"timingprofile":["100.00%  409.433      1 -total"," 71.07%  290.990      1 Template:Reflist"," 42.09%  172.314      5 Template:Cite_journal"," 15.14%   61.985      2 Template:ISBN"," 12.11%   49.575      1 Template:Short_description"," 11.28%   46.174      1 Template:Pagetype","  5.91%   24.201      2 Template:Catalog_lookup_link","  3.75%   15.357      2 Template:Error-small","  3.75%   15.347      1 Template:See_also","  3.01%   12.327      2 Template:Small"]},"scribunto":{"limitreport-timeusage":{"value":"0.173","limit":"10.000"},"limitreport-memusage":{"value":3655264,"limit":52428800}},"cachereport":{"origin":"mw1243","timestamp":"20190304012407","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Nonlinear dimensionality reduction","url":"https:\/\/en.wikipedia.org\/wiki\/Nonlinear_dimensionality_reduction#Manifold_learning_algorithms","sameAs":"http:\/\/www.wikidata.org\/entity\/Q7049464","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q7049464","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2003-09-01T14:54:56Z","dateModified":"2019-03-03T19:10:49Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fd\/Lle_hlle_swissroll.png"}</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":108,"wgHostname":"mw1270"});});</script>
	</body>
</html>
