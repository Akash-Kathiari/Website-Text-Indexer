<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Portal:Machine learning - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"Portal","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":100,"wgPageName":"Portal:Machine_learning","wgTitle":"Machine learning","wgCurRevisionId":882169711,"wgRevisionId":882169711,"wgArticleId":44942806,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Portals with short description","Single-page portals","All portals","Portals with titles not starting with a proper noun","Machine learning"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Portal:Machine_learning","wgRelevantArticleId":44942806,"wgRequestId":"XH6GXwpAIDEAAEmr1VcAAABM","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsReferencePreviews":false,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFIsPageContentModelEditable":true,"wgMFEnableFontChanger":true,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q58630879","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true,"oresWikiId":"enwiki","oresBaseUrl":"http://ores.discovery.wmnet:8081/","oresApiVersion":3});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.math.styles":"ready","mediawiki.page.gallery.styles":"ready","ext.categoryTree.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.math.scripts","mediawiki.page.gallery.slideshow","ext.scribunto.logs","ext.categoryTree","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.3d.styles%7Cext.categoryTree.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.page.gallery.styles%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.33.0-wmf.19"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Portal:Machine_learning&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Portal:Machine_learning&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Portal:Machine_learning"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-100 ns-subject mw-editable page-Portal_Machine_learning rootpage-Portal_Machine_learning skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Portal:Machine learning</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"></div>
				<div id="jump-to-nav"></div>				<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
				<a class="mw-jump-link" href="#p-search">Jump to search</a>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Wikipedia's portal for exploring content related to Machine learning</div>
<div class="portal-maintenance-status" style="display:none;">
<table class="plainlinks ombox ombox-notice" role="presentation"><tbody><tr><td class="mbox-image"><a href="/wiki/File:Darkgreen_flag_waving.svg" class="image"><img alt="Darkgreen flag waving.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/30px-Darkgreen_flag_waving.svg.png" decoding="async" width="30" height="32" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/45px-Darkgreen_flag_waving.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/60px-Darkgreen_flag_waving.svg.png 2x" data-file-width="249" data-file-height="268" /></a></td><td class="mbox-text"><span style="font-size:108%;"><b>Portal maintenance status:</b></span> <small>(September 2018)</small>
<ul><li>This portal has a <b>single page layout</b>. Any <a href="/wiki/Special:PrefixIndex/Portal:Machine_learning/" title="Special:PrefixIndex/Portal:Machine learning/">subpages</a> are likely no longer needed.</li></ul>
<span style="font-size:90%;">Please <a href="/wiki/Wikipedia:CAREFUL" class="mw-redirect" title="Wikipedia:CAREFUL">take care</a> when editing, especially if using <a href="/wiki/Wikipedia:ASSISTED" class="mw-redirect" title="Wikipedia:ASSISTED">automated editing software</a>.&#32;Learn how to <a href="/wiki/Template:Portal_maintenance_status#How_to_update_the_maintenance_information_for_a_portal" title="Template:Portal maintenance status">update the maintenance information here</a>.</span></td></tr></tbody></table></div>
<div id="portals-browsebar" class="hlist noprint" style="text-align: center;">
<dl><dt><a href="/wiki/Portal:Contents/Portals" title="Portal:Contents/Portals">Portal topics</a></dt>
<dd><a href="/wiki/Portal:Contents/Portals#Human_activities" title="Portal:Contents/Portals">Activities</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Culture_and_the_arts" title="Portal:Contents/Portals">Culture</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Geography_and_places" title="Portal:Contents/Portals">Geography</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Health_and_fitness" title="Portal:Contents/Portals">Health</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#History_and_events" title="Portal:Contents/Portals">History</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Mathematics_and_logic" title="Portal:Contents/Portals">Mathematics</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Natural_and_physical_sciences" title="Portal:Contents/Portals">Nature</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#People_and_self" title="Portal:Contents/Portals">People</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Philosophy_and_thinking" title="Portal:Contents/Portals">Philosophy</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Religion_and_belief_systems" title="Portal:Contents/Portals">Religion</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Society_and_social_sciences" title="Portal:Contents/Portals">Society</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Technology_and_applied_sciences" title="Portal:Contents/Portals">Technology</a></dd>
<dd><a href="/wiki/Special:RandomInCategory/All_portals" title="Special:RandomInCategory/All portals">Random portal</a></dd></dl>
</div> 
<div style="clear:both; width:100%">
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Introduction">Introduction</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0"> 
<p><b><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></b> (ML) is the <a href="/wiki/Branches_of_science" title="Branches of science">scientific study</a> of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> and <a href="/wiki/Statistical_model" title="Statistical model">statistical models</a> that <a href="/wiki/Computer_systems" class="mw-redirect" title="Computer systems">computer systems</a> use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. Machine learning algorithms build a mathematical model of sample data, known as "<a href="/wiki/Training_data" class="mw-redirect" title="Training data">training data</a>", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of <a href="/wiki/Email_filtering" title="Email filtering">email filtering</a>, detection of network intruders, and <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to <a href="/wiki/Computational_statistics" title="Computational statistics">computational statistics</a>, which focuses on making predictions using computers. The study of <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">mathematical optimization</a> delivers methods, theory and application domains to the field of machine learning. <a href="/wiki/Data_mining" title="Data mining">Data mining</a> is a field of study within machine learning, and focuses on <a href="/wiki/Exploratory_data_analysis" title="Exploratory data analysis">exploratory data analysis</a> through <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>. In its application across business problems, machine learning is also referred to as <a href="/wiki/Predictive_analytics" title="Predictive analytics">predictive analytics</a>.
</p>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b><a href="/wiki/Machine_learning" title="Machine learning">Read more...</a></b></div><div style="clear:both;"></div></div>
<div style="text-align:center; margin:0.25em auto 0.75em"><span class="noprint plainlinks purgelink"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;action=purge"><span title="Purge this page"><b>Refresh with new selections below</b> (purge)</span></a></span></div>
<style data-mw-deduplicate="TemplateStyles:r886281085">.mw-parser-output .flex-columns-container{clear:both;width:100%;display:flex;flex-wrap:wrap}.mw-parser-output .flex-columns-container>.flex-columns-column{float:left;width:50%;min-width:360px;padding:0 0.5em;box-sizing:border-box;flex:1;display:flex;flex-direction:column}@media screen and (max-width:393px){.mw-parser-output .flex-columns-container>.flex-columns-column{min-width:0}}.mw-parser-output .flex-columns-container>.flex-columns-column:first-child{padding-left:0}.mw-parser-output .flex-columns-container>.flex-columns-column:last-child{padding-right:0}@media screen and (max-width:720px){.mw-parser-output .flex-columns-container>.flex-columns-column{padding:0;width:100%}.mw-parser-output .flex-columns-container{display:block}}.mw-parser-output .flex-columns-container>.flex-columns-column>div{flex:1 0 auto}.mw-parser-output .flex-columns-container>.flex-columns-column>div.flex-columns-noflex{flex:0}</style><div class="flex-columns-container"><div class="flex-columns-column"><div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Selected_general_articles">Selected general articles</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<style data-mw-deduplicate="TemplateStyles:r886046835">.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div>div>span:nth-child(2){display:none}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div:nth-child(2){display:none}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div:nth-child(1){padding-bottom:0}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li:nth-child(n/**/+2){display:none}.mw-parser-output div.excerptSlideshow-container .gallery .gallerybox,.mw-parser-output div.excerptSlideshow-container .gallery .gallerybox div{width:100%!important;max-width:100%}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li:not(.gallerycarousel)>div>div:nth-child(1){display:none}</style><div class="excerptSlideshow-container" style="max-width:100%; margin:-4em auto;"><ul class="gallery mw-gallery-slideshow" data-showthumbnails="">
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">Vapnik–Chervonenkis theory</a></b> (also known as <b>VC theory</b>) was developed during 1960–1990 by <a href="/wiki/Vladimir_Vapnik" title="Vladimir Vapnik">Vladimir Vapnik</a> and <a href="/wiki/Alexey_Chervonenkis" title="Alexey Chervonenkis">Alexey Chervonenkis</a>. The theory is a form of <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a>, which attempts to explain the learning process from a statistical point of view.<br /><br />VC theory is related to <a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">statistical learning theory</a>  and to <a href="/wiki/Empirical_processes" class="mw-redirect" title="Empirical processes">empirical processes</a>.  <a href="/wiki/Richard_M._Dudley" title="Richard M. Dudley">Richard M. Dudley</a> and <a href="/wiki/Vladimir_Vapnik" title="Vladimir Vapnik">Vladimir Vapnik</a>, among others, have applied VC-theory to <a href="/wiki/Empirical_processes" class="mw-redirect" title="Empirical processes">empirical processes</a>. <b><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></b> (ERM) is a principle in <a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">statistical learning theory</a> which defines a family of <a href="/wiki/Machine_learning" title="Machine learning">learning algorithms</a> and is used to give theoretical bounds on their performance. <b><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="/wiki/File:Colored_neural_network.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" decoding="async" width="300" height="361" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/450px-Colored_neural_network.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/600px-Colored_neural_network.svg.png 2x" data-file-width="296" data-file-height="356" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Colored_neural_network.svg" class="internal" title="Enlarge"></a></div>An artificial neural network is an interconnected group of nodes, similar to the vast network of <a href="/wiki/Neuron" title="Neuron">neurons</a> in a <a href="/wiki/Brain" title="Brain">brain</a>. Here, each circular node represents an <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neuron</a> and an arrow represents a connection from the output of one artificial neuron to the input of another.</div></div></div><br /><b><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></b> (<b>ANN</b>) or <b>connectionist systems</b> are computing systems inspired by the <a href="/wiki/Biological_neural_network" class="mw-redirect" title="Biological neural network">biological neural networks</a> that constitute animal <a href="/wiki/Brain" title="Brain">brains</a>. The neural network itself is not an algorithm, but rather a framework for many different <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms to work together and process complex data inputs. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, in <a href="/wiki/Image_recognition" class="mw-redirect" title="Image recognition">image recognition</a>, they might learn to identify images that contain cats by analyzing example images that have been manually <a href="/wiki/Labeled_data" title="Labeled data">labeled</a> as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge about cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process.<br /><br />An ANN is based on a collection of connected units or nodes called <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neurons</a>, which loosely model the <a href="/wiki/Neuron" title="Neuron">neurons</a> in a biological brain. Each connection, like the <a href="/wiki/Synapse" title="Synapse">synapses</a> in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. <b><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a>, <b><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">probably approximately correct</a></b> (<b>PAC</b>) <b>learning</b> is a framework for mathematical analysis of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>. It was proposed in 1984 by <a href="/wiki/Leslie_Valiant" title="Leslie Valiant">Leslie Valiant</a>.<br /><br />In this framework, the learner receives samples and must select a generalization function (called the <i>hypothesis</i>) from a certain class of possible functions. The goal is that, with high probability (the "probably" part), the selected function will have low <a href="/wiki/Generalization_error" title="Generalization error">generalization error</a> (the "approximately correct" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or <a href="/wiki/Empirical_distribution_function" title="Empirical distribution function">distribution of the samples</a>. <b><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Graph_model.svg" class="image"><img alt="An example of a graphical model." src="//upload.wikimedia.org/wikipedia/commons/thumb/3/39/Graph_model.svg/220px-Graph_model.svg.png" decoding="async" width="220" height="259" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/39/Graph_model.svg/330px-Graph_model.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/39/Graph_model.svg/440px-Graph_model.svg.png 2x" data-file-width="123" data-file-height="145" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Graph_model.svg" class="internal" title="Enlarge"></a></div>An example of a graphical model. Each arrow indicates a dependency. In this example: D depends on A, D depends on B, D depends on C, C depends on B, and C depends on D.</div></div></div><br />A <b><a href="/wiki/Graphical_model" title="Graphical model">graphical model</a></b> or <b>probabilistic graphical model</b> (<b>PGM</b>) or <b>structured probabilistic model</b> is a <a href="/wiki/Probabilistic_model" class="mw-redirect" title="Probabilistic model">probabilistic model</a> for which a <a href="/wiki/Graph_(discrete_mathematics)" title="Graph (discrete mathematics)">graph</a> expresses the <a href="/wiki/Conditional_dependence" title="Conditional dependence">conditional dependence</a> structure between <a href="/wiki/Random_variable" title="Random variable">random variables</a>. They are commonly used in <a href="/wiki/Probability_theory" title="Probability theory">probability theory</a>, <a href="/wiki/Statistics" title="Statistics">statistics</a>—particularly <a href="/wiki/Bayesian_statistics" title="Bayesian statistics">Bayesian statistics</a>—and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>. <b><a href="/wiki/Graphical_model" title="Graphical model">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Cluster-2.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Cluster-2.svg/220px-Cluster-2.svg.png" decoding="async" width="220" height="147" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Cluster-2.svg/330px-Cluster-2.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Cluster-2.svg/440px-Cluster-2.svg.png 2x" data-file-width="601" data-file-height="402" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Cluster-2.svg" class="internal" title="Enlarge"></a></div>The result of a cluster analysis shown as the coloring of the squares into three clusters.</div></div></div><br /><b><a href="/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></b> or <b>clustering</b> is the task of grouping a set of objects in such a way that objects in the same group (called a <b>cluster</b>) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory <a href="/wiki/Data_mining" title="Data mining">data mining</a>, and a common technique for <a href="/wiki/Statistics" title="Statistics">statistical</a> <a href="/wiki/Data_analysis" title="Data analysis">data analysis</a>, used in many fields, including <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, <a href="/wiki/Image_analysis" title="Image analysis">image analysis</a>, <a href="/wiki/Information_retrieval" title="Information retrieval">information retrieval</a>, <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>, <a href="/wiki/Data_compression" title="Data compression">data compression</a>, and <a href="/wiki/Computer_graphics" title="Computer graphics">computer graphics</a>.<br /><br />Cluster analysis itself is not one specific <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small <a href="/wiki/Distance_function" class="mw-redirect" title="Distance function">distances</a> between cluster members, dense areas of the data space, intervals or particular <a href="/wiki/Statistical_distribution" class="mw-redirect" title="Statistical distribution">statistical distributions</a>. Clustering can therefore be formulated as a <a href="/wiki/Multi-objective_optimization" title="Multi-objective optimization">multi-objective optimization</a> problem. The appropriate clustering algorithm and parameter settings (including parameters such as the <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">distance function</a> to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of <a href="/wiki/Knowledge_discovery" class="mw-redirect" title="Knowledge discovery">knowledge discovery</a> or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties. <b><a href="/wiki/Cluster_analysis" title="Cluster analysis">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Restricted_Boltzmann_machine.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Restricted_Boltzmann_machine.svg/220px-Restricted_Boltzmann_machine.svg.png" decoding="async" width="220" height="234" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Restricted_Boltzmann_machine.svg/330px-Restricted_Boltzmann_machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Restricted_Boltzmann_machine.svg/440px-Restricted_Boltzmann_machine.svg.png 2x" data-file-width="310" data-file-height="330" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Restricted_Boltzmann_machine.svg" class="internal" title="Enlarge"></a></div>Diagram of a restricted Boltzmann machine with three visible units and four hidden units (no bias units).</div></div></div><br />A <b><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machine</a></b> (<b>RBM</b>) is a <a href="/wiki/Generative_model" title="Generative model">generative</a> <a href="/wiki/Stochastic_neural_network" title="Stochastic neural network">stochastic</a> <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> that can learn a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> over its set of inputs. <br /><br />RBMs were initially invented under the name <b>Harmonium</b> by <a href="/wiki/Paul_Smolensky" title="Paul Smolensky">Paul Smolensky</a> in 1986,<br />and rose to prominence after <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a> and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>,<br /><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>,<br /><a href="/wiki/Collaborative_filtering" title="Collaborative filtering">collaborative filtering</a>,  <a href="/wiki/Feature_learning" title="Feature learning">feature learning</a>,<br /><a href="/wiki/Topic_model" title="Topic model">topic modelling</a><br />and even <a href="/wiki/Many-body_problem" title="Many-body problem">many body quantum mechanics</a>. They can be trained in either <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a> or <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> ways, depending on the task. <b><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Linear discriminant analysis</a></b> (<b>LDA</b>), <b>normal discriminant analysis</b> (<b>NDA</b>), or <b>discriminant function analysis</b> is a generalization of <b>Fisher's linear discriminant</b>, a method used in <a href="/wiki/Statistics" title="Statistics">statistics</a>, <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> to find a <a href="/wiki/Linear_combination" title="Linear combination">linear combination</a> of <a href="/wiki/Features_(pattern_recognition)" class="mw-redirect" title="Features (pattern recognition)">features</a> that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, or, more commonly, for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> before later <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>.<br /><br />LDA is closely related to <a href="/wiki/Analysis_of_variance" title="Analysis of variance">analysis of variance</a> (ANOVA) and <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, which also attempt to express one <a href="/wiki/Dependent_variable" class="mw-redirect" title="Dependent variable">dependent variable</a> as a linear combination of other features or measurements. However, ANOVA uses <a href="/wiki/Categorical_variable" title="Categorical variable">categorical</a> <a href="/wiki/Independent_variables" class="mw-redirect" title="Independent variables">independent variables</a> and a <a href="/wiki/Continuous_variable" class="mw-redirect" title="Continuous variable">continuous</a> <a href="/wiki/Dependent_variable" class="mw-redirect" title="Dependent variable">dependent variable</a>, whereas discriminant analysis has continuous <a href="/wiki/Independent_variables" class="mw-redirect" title="Independent variables">independent variables</a> and a categorical dependent variable (<i>i.e.</i> the class label). <a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a> and <a href="/wiki/Probit_regression" class="mw-redirect" title="Probit regression">probit regression</a> are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method. <b><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:196px;"><a href="/wiki/File:Example_of_unlabeled_data_in_semisupervised_learning.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png/194px-Example_of_unlabeled_data_in_semisupervised_learning.png" decoding="async" width="194" height="225" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png/291px-Example_of_unlabeled_data_in_semisupervised_learning.png 1.5x, //upload.wikimedia.org/wikipedia/commons/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png 2x" data-file-width="388" data-file-height="449" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Example_of_unlabeled_data_in_semisupervised_learning.png" class="internal" title="Enlarge"></a></div>An example of the influence of unlabeled data in semi-supervised learning.  The top panel shows a decision boundary we might adopt after seeing only one positive (white circle) and one negative (black circle) example.  The bottom panel shows a decision boundary we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles).  This could be viewed as performing <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a> and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.</div></div></div><br /><br /><b><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></b> is a class of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> tasks and techniques that also make use of unlabeled <a href="/wiki/Data" title="Data">data</a> for training – typically a small amount of <a href="/wiki/Labeled_data" title="Labeled data">labeled data</a> with a large amount of unlabeled data.  Semi-supervised learning falls between <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> (without any labeled training data) and <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> (with completely labeled training data).  Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy over unsupervised learning (where no data is labeled), but without the time and costs needed for supervised learning (where all data is labeled). The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value.  Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.<br /><br />As in the supervised learning framework, we are given a set of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="l"/></span> <a href="/wiki/Independent_identically_distributed" class="mw-redirect" title="Independent identically distributed">independently identically distributed</a> examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{1},\dots ,x_{l}\in X}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msub>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>X</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{1},\dots ,x_{l}\in X}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76da26bfd12e40809f4b2dae37ecca34ad1c825c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:14.435ex; height:2.509ex;" alt="x_{1},\dots ,x_{l}\in X"/></span> with corresponding labels <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y_{1},\dots ,y_{l}\in Y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msub>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>Y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y_{1},\dots ,y_{l}\in Y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b376abba952ea1dca784912adb9a3bfd006eb6b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:13.847ex; height:2.509ex;" alt="y_{1},\dots ,y_{l}\in Y"/></span>.  Additionally, we are given <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle u}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>u</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle u}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e6bb763d22c20916ed4f0bb6bd49d7470cffd8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="u"/></span> unlabeled examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>+</mo>
            <mi>u</mi>
          </mrow>
        </msub>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>X</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60a05a61c90d36f1a9def946c7dd83e826162b26" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:18.423ex; height:2.509ex;" alt="x_{l+1},\dots ,x_{l+u}\in X"/></span>.  Semi-supervised learning attempts to make use of this combined information to surpass the <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning. <b><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, <b><a href="/wiki/Independent_component_analysis" title="Independent component analysis">independent component analysis</a></b> (<b>ICA</b>) is a computational method for separating a <a href="/wiki/Multivariate_statistics" title="Multivariate statistics">multivariate</a> signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are <a href="/wiki/Statistical_independence" class="mw-redirect" title="Statistical independence">statistically independent</a> from each other. ICA is a special case of <a href="/wiki/Blind_source_separation" class="mw-redirect" title="Blind source separation">blind source separation</a>. A common example application is the "<a href="/wiki/Cocktail_party_problem" class="mw-redirect" title="Cocktail party problem">cocktail party problem</a>" of listening in on one person's speech in a noisy room. <b><a href="/wiki/Independent_component_analysis" title="Independent component analysis">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Canonical_correlation" title="Canonical correlation">canonical-correlation analysis</a></b> (<b>CCA</b>, also called 'Canonical Variates Analysis') is a way of inferring information from <a href="/wiki/Cross-covariance_matrix" title="Cross-covariance matrix">cross-covariance matrices</a>. If we have two vectors <i>X</i>&#160;=&#160;(<i>X</i><sub>1</sub>,&#160;...,&#160;<i>X</i><sub><i>n</i></sub>) and <i>Y</i>&#160;=&#160;(<i>Y</i><sub>1</sub>,&#160;...,&#160;<i>Y</i><sub><i>m</i></sub>)  of <a href="/wiki/Random_variable" title="Random variable">random variables</a>, and there are <a href="/wiki/Correlation" class="mw-redirect" title="Correlation">correlations</a> among the variables, then canonical-correlation analysis will find linear combinations of <i>X</i> and <i>Y</i> which have maximum correlation with each other. T. R. Knapp notes that "virtually all of the commonly encountered <a href="/wiki/Parametric_statistics" title="Parametric statistics">parametric tests</a> of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables." The method was first introduced by <a href="/wiki/Harold_Hotelling" title="Harold Hotelling">Harold Hotelling</a> in 1936, although in the context of <a href="/wiki/Angles_between_flats" title="Angles between flats">angles between flats</a> the mathematical concept was published by Jordan in 1875. <b><a href="/wiki/Canonical_correlation" title="Canonical correlation">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bootstrap aggregating</a></b>, also called <b>bagging</b>, is a <a href="/wiki/Ensemble_learning" title="Ensemble learning">machine learning ensemble</a> <a href="/wiki/Meta-algorithm" class="mw-redirect" title="Meta-algorithm">meta-algorithm</a> designed to improve the stability and accuracy of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms used in <a href="/wiki/Statistical_classification" title="Statistical classification">statistical classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. It also reduces <a href="/wiki/Variance" title="Variance">variance</a> and helps to avoid <a href="/wiki/Overfitting" title="Overfitting">overfitting</a>. Although it is usually applied to <a href="/wiki/Decision_tree_learning" title="Decision tree learning">decision tree</a> methods, it can be used with any type of method. Bagging is a special case of the <a href="/wiki/Ensemble_learning" title="Ensemble learning">model averaging</a> approach. <b><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">T-distributed Stochastic Neighbor Embedding (t-SNE)</a></b> is a <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithm for <a href="/wiki/Data_visualization" title="Data visualization">visualization</a> developed by Laurens van der Maaten and <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a>. It is a <a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">nonlinear dimensionality reduction</a> technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.<br /><br />The t-SNE algorithm comprises two main stages. First, t-SNE constructs a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the <a href="/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback–Leibler divergence">Kullback–Leibler divergence</a> between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> between objects as the base of its similarity metric, this should be changed as appropriate. <b><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, the <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-nearest neighbors algorithm</a></b> (<b><i>k</i>-NN</b>) is a <a href="/wiki/Non-parametric_statistics" class="mw-redirect" title="Non-parametric statistics">non-parametric</a> method used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. In both cases, the input consists of the <i>k</i> closest training examples in the <a href="/wiki/Feature_space" class="mw-redirect" title="Feature space">feature space</a>. The output depends on whether <i>k</i>-NN is used for classification or regression:<br /><br />:* In <i>k-NN classification</i>, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i>&#160;=&#160;1, then the object is simply assigned to the class of that single nearest neighbor. <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></b> is a term used for <a href="/wiki/Hebbian_learning" class="mw-redirect" title="Hebbian learning">Hebbian learning</a>, associated to learning without a teacher, also known as <a href="/wiki/Self-organization" title="Self-organization">self-organization</a> and a method of modelling the probability density of inputs. <br /><br />The <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> as a branch of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. <b><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>, a <b><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural network</a></b> (<b>CNN</b>, or <b>ConvNet</b>) is a class of <a href="/wiki/Deep_neural_network" class="mw-redirect" title="Deep neural network">deep neural networks</a>, most commonly applied to analyzing visual imagery.<br /><br />CNNs use a variation of <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a> designed to require minimal <a href="/wiki/Data_pre-processing" title="Data pre-processing">preprocessing</a>. They are also known as <b>shift invariant</b> or <b>space invariant artificial neural networks</b> (<b>SIANN</b>), based on their shared-weights architecture and <a href="/wiki/Translation_invariance" class="mw-redirect" title="Translation invariance">translation invariance</a> characteristics. <b><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Support-vector_machine" title="Support-vector machine">support-vector machines</a></b> (<b>SVMs</b>, also <b>support-vector networks</b>) are <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> models with associated learning <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> that analyze data used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>.  Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-<a href="/wiki/Probabilistic_classification" title="Probabilistic classification">probabilistic</a> <a href="/wiki/Binary_classifier" class="mw-redirect" title="Binary classifier">binary</a> <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a> (although methods such as <a href="/wiki/Platt_scaling" title="Platt scaling">Platt scaling</a> exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.<br /><br />In addition to performing <a href="/wiki/Linear_classifier" title="Linear classifier">linear classification</a>, SVMs can efficiently perform a non-linear classification using what is called the <a href="/wiki/Kernel_trick" class="mw-redirect" title="Kernel trick">kernel trick</a>, implicitly mapping their inputs into high-dimensional feature spaces. <b><a href="/wiki/Support-vector_machine" title="Support-vector machine">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, the <b><a href="/wiki/Perceptron" title="Perceptron">perceptron</a></b> is an algorithm for <a href="/wiki/Supervised_classification" class="mw-redirect" title="Supervised classification">supervised learning</a> of <a href="/wiki/Binary_classification" title="Binary classification">binary classifiers</a>.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, i.e. a classification algorithm that makes its predictions based on a <a href="/wiki/Linear_predictor_function" title="Linear predictor function">linear predictor function</a> combining a set of weights with the <a href="/wiki/Feature_vector" class="mw-redirect" title="Feature vector">feature vector</a>. <b><a href="/wiki/Perceptron" title="Perceptron">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br /><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" decoding="async" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" /></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
			
		
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:352px;"><a href="/wiki/File:Autoencoder_structure.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png" decoding="async" width="350" height="262" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/525px-Autoencoder_structure.png 1.5x, //upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png 2x" data-file-width="677" data-file-height="506" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Autoencoder_structure.png" class="internal" title="Enlarge"></a></div>Schematic structure of an autoencoder with 3 fully connected hidden layers.</div></div></div><br />An <b><a href="/wiki/Autoencoder" title="Autoencoder">autoencoder</a></b> is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> used to learn <a href="/wiki/Feature_learning" title="Feature learning">efficient data codings</a> in an <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>, by training the network to ignore signal “noise.” Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Recently, the autoencoder concept has become more widely used for learning <a href="/wiki/Generative_model" title="Generative model">generative models</a> of data. Some of the most powerful AI in the 2010s have involved sparse autoencoders stacked inside of deep neural networks. <b><a href="/wiki/Autoencoder" title="Autoencoder">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">The following <a href="/wiki/Outline_(list)" title="Outline (list)">outline</a> is provided as an overview of and topical guide to <b><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">machine learning</a></b>. <a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> is a subfield of <a href="/wiki/Soft_computing" title="Soft computing">soft computing</a> within <a href="/wiki/Computer_science" title="Computer science">computer science</a> that evolved from the study of <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a> in <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. In 1959, <a href="/wiki/Arthur_Samuel" title="Arthur Samuel">Arthur Samuel</a> defined machine learning as a "field of study that gives computers the ability to learn without being explicitly programmed". Machine learning explores the study and construction of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> that can <a href="/wiki/Learning" title="Learning">learn</a> from and make predictions on <a href="/wiki/Data" title="Data">data</a>. Such algorithms operate by building a <a href="/wiki/Mathematical_model" title="Mathematical model">model</a> from an example <a href="/wiki/Training_set" class="mw-redirect" title="Training set">training set</a> of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions. <b><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b><a href="/wiki/Online_machine_learning" title="Online machine learning">online machine learning</a></b> is a method of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of <a href="/wiki/Out-of-core" class="mw-redirect" title="Out-of-core">out-of-core</a> algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g. <a href="/wiki/Stock_market_prediction" title="Stock market prediction">stock price prediction</a>.<br />Online learning algorithms may be prone to <a href="/wiki/Catastrophic_interference" title="Catastrophic interference">catastrophic interference</a>, a problem that can be addressed by <a href="/wiki/Incremental_learning" title="Incremental learning">incremental learning</a> approaches. <b><a href="/wiki/Online_machine_learning" title="Online machine learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Feature_learning" title="Feature learning">feature learning</a></b> or <b>representation learning</b> is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual <a href="/wiki/Feature_engineering" title="Feature engineering">feature engineering</a> and allows a machine to both learn the features  and use them to perform  a specific task.<br /><br />Feature learning is motivated by the fact that machine learning tasks such as <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. <b><a href="/wiki/Feature_learning" title="Feature learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></b> (or <b>grammatical inference</b>) is the process in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> of learning a <a href="/wiki/Formal_grammar" title="Formal grammar">formal grammar</a> (usually as a collection of <i>re-write rules</i> or <i><a href="/wiki/Productions_(computer_science)" class="mw-redirect" title="Productions (computer science)">productions</a></i> or alternatively as a <a href="/wiki/Finite_state_machine" class="mw-redirect" title="Finite state machine">finite state machine</a> or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs. <b><a href="/wiki/Grammar_induction" title="Grammar induction">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></b> (also known as <b>deep structured learning</b> or <b>hierarchical learning</b>) is part of a broader family of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> methods based on <a href="/wiki/Learning_representation" class="mw-redirect" title="Learning representation">learning data representations</a>, as opposed to task-specific algorithms. Learning can be <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a>, <a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-supervised</a> or <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a>.<br /><br />Deep learning architectures such as <a href="#Deep_neural_networks">deep neural networks</a>, <a href="/wiki/Deep_belief_network" title="Deep belief network">deep belief networks</a> and <a href="/wiki/Recurrent_neural_networks" class="mw-redirect" title="Recurrent neural networks">recurrent neural networks</a> have been applied to fields including <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, <a href="/wiki/Automatic_speech_recognition" class="mw-redirect" title="Automatic speech recognition">speech recognition</a>, <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>, audio recognition, social network filtering, <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a>, <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>, <a href="/wiki/Drug_design" title="Drug design">drug design</a>, medical image analysis, material inspection and <a href="/wiki/Board_game" title="Board game">board game</a> programs, where they have produced results comparable to and in some cases superior to human experts. <b><a href="/wiki/Deep_learning" title="Deep learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b>Density-based spatial clustering of applications with noise</b> (<b><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></b>) is a <a href="/wiki/Data_clustering" class="mw-redirect" title="Data clustering">data clustering</a> algorithm proposed by Martin Ester, <a href="/wiki/Hans-Peter_Kriegel" title="Hans-Peter Kriegel">Hans-Peter Kriegel</a>, Jörg Sander and Xiaowei Xu in 1996.<br />It is a <a href="/wiki/Cluster_analysis#Density-based_clustering" title="Cluster analysis">density-based clustering</a> algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many <a href="/wiki/Fixed-radius_near_neighbors" title="Fixed-radius near neighbors">nearby neighbors</a>), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).<br />DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.<br /><br />In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD. <b><a href="/wiki/DBSCAN" title="DBSCAN">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="/wiki/File:Synapse_Self-Organizing_Map.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/70/Synapse_Self-Organizing_Map.png/300px-Synapse_Self-Organizing_Map.png" decoding="async" width="300" height="282" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/70/Synapse_Self-Organizing_Map.png/450px-Synapse_Self-Organizing_Map.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/70/Synapse_Self-Organizing_Map.png/600px-Synapse_Self-Organizing_Map.png 2x" data-file-width="759" data-file-height="713" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Synapse_Self-Organizing_Map.png" class="internal" title="Enlarge"></a></div>A self-organizing map showing <a href="/wiki/United_States_Congress" title="United States Congress">U.S. Congress</a> voting patterns. The input data was a table with a row for each member of Congress, and columns for certain votes containing each member's yes/no/abstain vote. The SOM algorithm arranged these members in a two-dimensional grid placing similar members closer together. <b>The first plot</b> shows the grouping when the data are split into two clusters. <b>The second plot</b> shows average distance to neighbours: larger distances are darker. <b>The third plot</b> predicts <a href="/wiki/Republican_Party_(United_States)" title="Republican Party (United States)">Republican</a> (red) or <a href="/wiki/Democratic_Party_(United_States)" title="Democratic Party (United States)">Democratic</a> (blue) party membership. <b>The other plots</b> each overlay the resulting map with predicted values on an input dimension: red means a predicted 'yes' vote on that bill, blue means a 'no' vote. The plot was created in <a href="/wiki/Peltarion_Synapse" title="Peltarion Synapse">Synapse</a>.</div></div></div><br />A <b><a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a></b> (<b>SOM</b>) or <b>self-organizing feature map</b> (<b>SOFM</b>) is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> (ANN) that is trained using <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a <b>map</b>, and is therefore a method to do <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>. Self-organizing maps differ from other artificial neural networks as they apply <a href="/wiki/Competitive_learning" title="Competitive learning">competitive learning</a> as opposed to error-correction learning (such as <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> with <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a>), and in the sense that they use a neighborhood function to preserve the <a href="/wiki/Topology" title="Topology">topological</a> properties of the input space.<br /><br />This makes SOMs useful for <a href="/wiki/Scientific_visualization" title="Scientific visualization">visualization</a> by creating low-dimensional views of high-dimensional data, akin to <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a>. The artificial neural network introduced by the <a href="/wiki/Finland" title="Finland">Finnish</a> professor <a href="/wiki/Teuvo_Kohonen" title="Teuvo Kohonen">Teuvo Kohonen</a> in the 1980s is sometimes called a <b>Kohonen map</b> or <b>network</b>. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and <a href="/wiki/Morphogenesis" title="Morphogenesis">morphogenesis</a> models dating back to <a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a> in the  1950s. <b><a href="/wiki/Self-organizing_map" title="Self-organizing map">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a>, the <b>logistic model</b> (or <b>logit model</b>) is a widely used <a href="/wiki/Statistical_model" title="Statistical model">statistical model</a> that, in its basic form, uses a <a href="/wiki/Logistic_function" title="Logistic function">logistic function</a> to model a <a href="/wiki/Binary_variable" class="mw-redirect" title="Binary variable">binary</a> <a href="/wiki/Dependent_variable" class="mw-redirect" title="Dependent variable">dependent variable</a>; many more complex <a href="#Extensions">extensions</a> exist. In <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, <b><a href="/wiki/Logistic_regression" title="Logistic regression">logistic regression</a></b> (or <b>logit regression</b>) is <a href="/wiki/Estimation_theory" title="Estimation theory">estimating</a> the parameters of a logistic model; it is a form of <a href="/wiki/Binomial_regression" title="Binomial regression">binomial regression</a>. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail, win/lose, alive/dead or healthy/sick; these are represented by an <a href="/wiki/Indicator_variable" class="mw-redirect" title="Indicator variable">indicator variable</a>, where the two values are labeled "0" and "1". In the logistic model, the <a href="/wiki/Log-odds" class="mw-redirect" title="Log-odds">log-odds</a> (the <a href="/wiki/Logarithm" title="Logarithm">logarithm</a> of the <a href="/wiki/Odds" title="Odds">odds</a>) for the value labeled "1" is a <a href="/wiki/Linear_function_(calculus)" title="Linear function (calculus)">linear combination</a> of one or more <a href="/wiki/Independent_variable" class="mw-redirect" title="Independent variable">independent variables</a> ("predictors"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a <a href="/wiki/Continuous_variable" class="mw-redirect" title="Continuous variable">continuous variable</a> (any real value). The corresponding <a href="/wiki/Probability" title="Probability">probability</a> of the value labeled "1" can vary between 0 (certainly the value "0") and 1 (certainly the value "1"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The <a href="/wiki/Unit_of_measurement" title="Unit of measurement">unit of measurement</a> for the log-odds scale is called a <i><a href="/wiki/Logit" title="Logit">logit</a></i>, from <i><b>log</b>istic un<b>it</b></i>, hence the alternative names. Analogous models with a different <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> instead of the logistic function can also be used, such as the <a href="/wiki/Probit_model" title="Probit model">probit model</a>; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a <i>constant</i> rate, with each dependent variable having its own parameter; for a binary independent variable this generalizes the <a href="/wiki/Odds_ratio" title="Odds ratio">odds ratio</a>.<br /><br />Logistic regression was developed by statistician <a href="/wiki/David_Cox_(statistician)" title="David Cox (statistician)">David Cox</a> in 1958. The binary logistic regression model has <a href="#Extensions">extensions</a> to more than two levels of the dependent variable: <a href="/wiki/Categorical_variable" title="Categorical variable">categorical</a> outputs with more than two values are modelled by <a href="/wiki/Multinomial_logistic_regression" title="Multinomial logistic regression">multinomial logistic regression</a>, and if the multiple categories are <a href="/wiki/Level_of_measurement#Ordinal_type" title="Level of measurement">ordered</a>, by <a href="/wiki/Ordinal_logistic_regression" class="mw-redirect" title="Ordinal logistic regression">ordinal logistic regression</a>, for example the proportional odds ordinal logistic model. The model itself simply models probability of output in terms of input, and does not perform <a href="/wiki/Statistical_classification" title="Statistical classification">statistical classification</a> (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a <a href="/wiki/Binary_classifier" class="mw-redirect" title="Binary classifier">binary classifier</a>. The coefficients are generally not computed by a closed-form expression, unlike <a href="/wiki/Linear_least_squares_(mathematics)" class="mw-redirect" title="Linear least squares (mathematics)">linear least squares</a>; see <a href="#Model_fitting">§&#160;Model fitting</a>. <b><a href="/wiki/Logistic_regression" title="Logistic regression">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></b> is a <a href="/wiki/Non-parametric" class="mw-redirect" title="Non-parametric">non-parametric</a> <a href="/wiki/Feature_space" class="mw-redirect" title="Feature space">feature-space</a> analysis technique for locating the maxima of a <a href="/wiki/Density_function" class="mw-redirect" title="Density function">density function</a>, a so-called <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">mode</a>-seeking algorithm. Application domains include <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> and <a href="/wiki/Image_processing" class="mw-redirect" title="Image processing">image processing</a>. <b><a href="/wiki/Mean_shift" title="Mean shift">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Data_mining" title="Data mining">data mining</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">hierarchical clustering</a></b> (also called <b>hierarchical cluster analysis</b> or <b>HCA</b>) is a method of <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> which seeks to build a <a href="/wiki/Hierarchy" title="Hierarchy">hierarchy</a> of clusters. Strategies for hierarchical clustering generally fall into two types:<ul><li> <b>Agglomerative</b>: This is a "<a href="/wiki/Top-down_and_bottom-up_design" title="Top-down and bottom-up design">bottom-up</a>" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</li><li> <b>Divisive</b>: This is a "<a href="/wiki/Top-down_and_bottom-up_design" title="Top-down and bottom-up design">top-down</a>" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</li></ul><br /><br />In general, the merges and splits are determined in a <a href="/wiki/Greedy_algorithm" title="Greedy algorithm">greedy</a> manner. The results of hierarchical clustering are usually presented in a <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a>. <b><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:362px;"><a href="/wiki/File:EM_Clustering_of_Old_Faithful_data.gif" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif" decoding="async" width="360" height="309" class="thumbimage" data-file-width="360" data-file-height="309" /></a>  <div class="thumbcaption">EM clustering of <a href="/wiki/Old_Faithful" title="Old Faithful">Old Faithful</a> eruption data. The random initial model (which, due to the different scales of the axes, appears to be two very flat and wide spheres) is fit to the observed data. In the first iterations, the model changes substantially, but then converges to the two modes of the <a href="/wiki/Geyser" title="Geyser">geyser</a>. Visualized using <a href="/wiki/ELKI" title="ELKI">ELKI</a>.</div></div></div><br />In <a href="/wiki/Statistics" title="Statistics">statistics</a>, an <b><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">expectation–maximization</a></b> (<b>EM</b>) <b>algorithm</b> is an <a href="/wiki/Iterative_method" title="Iterative method">iterative method</a> to find <a href="/wiki/Maximum_likelihood" class="mw-redirect" title="Maximum likelihood">maximum likelihood</a> or <a href="/wiki/Maximum_a_posteriori" class="mw-redirect" title="Maximum a posteriori">maximum a posteriori</a> (MAP) estimates of <a href="/wiki/Parameter" title="Parameter">parameters</a> in <a href="/wiki/Statistical_model" title="Statistical model">statistical models</a>, where the model depends on unobserved <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a>. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the <a href="/wiki/Likelihood_function#Log-likelihood" title="Likelihood function">log-likelihood</a> evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the <i>E</i> step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step. <b><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></b> is a <a href="/wiki/Ensemble_learning" title="Ensemble learning">machine learning ensemble</a> <a href="/wiki/Meta-algorithm" class="mw-redirect" title="Meta-algorithm">meta-algorithm</a> for primarily reducing <a href="/wiki/Supervised_learning#Bias-variance_tradeoff" title="Supervised learning">bias</a>, and also variance in <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by <a href="/wiki/Michael_Kearns_(computer_scientist)" title="Michael Kearns (computer scientist)">Kearns</a> and <a href="/wiki/Leslie_Valiant" title="Leslie Valiant">Valiant</a> (1988, 1989): "Can a set of <b>weak learners</b> create a single <b>strong learner</b>?" A weak learner is defined to be a <a href="/wiki/Classification_(machine_learning)" class="mw-redirect" title="Classification (machine learning)">classifier</a> that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.<br /><br /><a href="/wiki/Robert_Schapire" title="Robert Schapire">Robert Schapire</a>'s affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, most notably leading to the development of boosting. <b><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Ensemble_learning" title="Ensemble learning">ensemble methods</a></b> use multiple learning algorithms to obtain better <a href="/wiki/Predictive_inference" title="Predictive inference">predictive performance</a> than could be obtained from any of the constituent learning algorithms alone.<br />Unlike a <a href="/wiki/Statistical_ensemble" class="mw-redirect" title="Statistical ensemble">statistical ensemble</a> in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives. <b><a href="/wiki/Ensemble_learning" title="Ensemble learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Q-learning" title="Q-learning"><i>Q</i>-learning</a></b> is a <a href="/wiki/Model-free_(reinforcement_learning)" title="Model-free (reinforcement learning)">model-free</a> <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation "model-free") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.<br /><br />For any finite <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a> (FMDP), <i>Q</i>-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and  all successive steps, starting from the current state. <i>Q</i>-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. "Q" names the function that returns the reward used to provide the reinforcement and can be said to stand for the "quality" of an action taken in a given state. <b><a href="/wiki/Q-learning" title="Q-learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></b> (balanced iterative reducing and clustering using hierarchies) is an unsupervised <a href="/wiki/Data_mining" title="Data mining">data mining</a> algorithm used to perform <a href="/wiki/Data_clustering" class="mw-redirect" title="Data clustering">hierarchical clustering</a> over particularly large data-sets. An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric <a href="/wiki/Data_point" class="mw-redirect" title="Data point">data points</a> in an attempt to produce the best quality clustering for a given set of resources (memory and <a href="/wiki/Time_constraint" title="Time constraint">time constraints</a>). In most cases, BIRCH only requires a single scan of the database.<br /><br />Its inventors claim BIRCH to be the "first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively", beating <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a> by two months. The algorithm received the SIGMOD 10 year test of time award in 2006. <b><a href="/wiki/BIRCH" title="BIRCH">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:172px;"><a href="/wiki/File:The_LSTM_cell.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/170px-The_LSTM_cell.png" decoding="async" width="170" height="112" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/255px-The_LSTM_cell.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/340px-The_LSTM_cell.png 2x" data-file-width="2014" data-file-height="1322" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:The_LSTM_cell.png" class="internal" title="Enlarge"></a></div>The Long Short-Term Memory (LSTM) cell can process data sequentially and keep its hidden state through time.</div></div></div><br /><br /><b><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory</a></b> (<b>LSTM</b>) is an artificial <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> (RNN) architecture used in the field of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>. Unlike standard <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural networks</a>, LSTM has feedback connections that make it a "general purpose computer" (that is, it can compute anything that a <a href="/wiki/Turing_machine" title="Turing machine">Turing machine</a> can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a> or <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>.<br /><a href="/wiki/Bloomberg_Business_Week" class="mw-redirect" title="Bloomberg Business Week">Bloomberg Business Week</a> wrote: "These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music."<br /><br />A common LSTM unit is composed of a <b>cell</b>, an <b>input gate</b>, an <b>output gate</b> and a <b>forget gate</b>. The cell remembers values over arbitrary time intervals and the three <i>gates</i> regulate the flow of information into and out of the cell. <b><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">A <b><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a></b> (<b>RNN</b>) is a class of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> where connections between nodes form a <a href="/wiki/Directed_graph" title="Directed graph">directed graph</a> along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike <a href="/wiki/Feedforward_neural_networks" class="mw-redirect" title="Feedforward neural networks">feedforward neural networks</a>, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a> or <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>.<br /><br />The term "recurrent neural network" is used indiscriminately to refer to two broad classes of networks with a similar general structure, where one is <a href="/wiki/Finite_impulse_response" title="Finite impulse response">finite impulse</a> and the other is <a href="/wiki/Infinite_impulse_response" title="Infinite impulse response">infinite impulse</a>. Both classes of networks exhibit temporal <a href="/wiki/Dynamic_system" class="mw-redirect" title="Dynamic system">dynamic behavior</a>. A finite impulse recurrent network is a <a href="/wiki/Directed_acyclic_graph" title="Directed acyclic graph">directed acyclic graph</a> that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a <a href="/wiki/Directed_cyclic_graph" class="mw-redirect" title="Directed cyclic graph">directed cyclic graph</a> that can not be unrolled. <b><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>, the <b><a href="/wiki/Local_outlier_factor" title="Local outlier factor">local outlier factor</a></b> (<b>LOF</b>) is an algorithm proposed by Markus M. Breunig, <a href="/wiki/Hans-Peter_Kriegel" title="Hans-Peter Kriegel">Hans-Peter Kriegel</a>, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.<br /><br />LOF shares some concepts with <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a> and <a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a> such as the concepts of "core distance" and "reachability distance", which are used for local density estimation. <b><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Mathematics" title="Mathematics">mathematics</a>, a <b><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance Vector Machine (RVM)</a></b> is a <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> technique that uses <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a> to obtain <a href="/wiki/Occam%27s_razor" title="Occam&#39;s razor">parsimonious</a> solutions for <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> and <a href="/wiki/Probabilistic_classification" title="Probabilistic classification">probabilistic classification</a>.<br />The RVM has an identical functional form to the <a href="/wiki/Support_vector_machine" class="mw-redirect" title="Support vector machine">support vector machine</a>, but provides probabilistic classification.<br /><br />It is actually equivalent to a <a href="/wiki/Gaussian_process" title="Gaussian process">Gaussian process</a> model with <a href="/wiki/Covariance_function" title="Covariance function">covariance function</a>:<br />:<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>N</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <msub>
              <mi>&#x03B1;<!-- α --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </msub>
          </mfrac>
        </mrow>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>,</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mo>&#x2032;</mo>
        </msup>
        <mo>,</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a9b6842d3fb3de1fc0b5cd93878ce056ca7997f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:34.51ex; height:7.676ex;" alt="k(\mathbf{x},\mathbf{x&#039;}) = \sum_{j=1}^N \frac{1}{\alpha_j} \varphi(\mathbf{x},\mathbf{x}_j)\varphi(\mathbf{x}&#039;,\mathbf{x}_j) "/></span><br />where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \varphi }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C6;<!-- φ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \varphi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33ee699558d09cf9d653f6351f9fda0b2f4aaa3e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.52ex; height:2.176ex;" alt="\varphi "/></span> is the <a href="/wiki/Kernel_function" class="mw-redirect" title="Kernel function">kernel function</a> (usually Gaussian), <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \alpha _{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03B1;<!-- α --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \alpha _{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/293a364991ab1ee55c25b0f60fd9e52af7b7dbde" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.397ex; height:2.343ex;" alt="\alpha _{j}"/></span> are the variances of the prior on the weight vector<br /><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w\sim N(0,\alpha ^{-1}I)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>w</mi>
        <mo>&#x223C;<!-- ∼ --></mo>
        <mi>N</mi>
        <mo stretchy="false">(</mo>
        <mn>0</mn>
        <mo>,</mo>
        <msup>
          <mi>&#x03B1;<!-- α --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mi>I</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w\sim N(0,\alpha ^{-1}I)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f4348b3f595d38d41a8dee315d073be107af558" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:15.824ex; height:3.176ex;" alt="w \sim N(0,\alpha^{-1}I)"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>N</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ceacccf86726a1ba6acfd40e67538e3e7c2336ec" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:10.746ex; height:2.009ex;" alt="\mathbf{x}_1,\ldots,\mathbf{x}_N"/></span> are the input vectors of the <a href="/wiki/Training_set" class="mw-redirect" title="Training set">training set</a>. <b><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov Model</a></b> (<b>HMM</b>) is a <a href="/wiki/Statistical_model" title="Statistical model">statistical</a> <a href="/wiki/Markov_model" title="Markov model">Markov model</a> in which the system being modeled is assumed to be a <a href="/wiki/Markov_process" class="mw-redirect" title="Markov process">Markov process</a> with unobserved (i.e. <i>hidden</i>) states.<br /><br />The hidden Markov model can be represented as the simplest <a href="/wiki/Dynamic_Bayesian_network" title="Dynamic Bayesian network">dynamic Bayesian network</a>. The mathematics behind the HMM were developed by <a href="/wiki/Leonard_E._Baum" title="Leonard E. Baum">L. E. Baum</a> and coworkers.<br /> HMM is closely related to earlier work on the optimal nonlinear <a href="/wiki/Filtering_problem_(stochastic_processes)" title="Filtering problem (stochastic processes)">filtering problem</a> by <a href="/wiki/Ruslan_L._Stratonovich" class="mw-redirect" title="Ruslan L. Stratonovich">Ruslan L. Stratonovich</a>, who was the first to describe the <a href="/wiki/Forward%E2%80%93backward_algorithm" title="Forward–backward algorithm">forward-backward procedure</a>. <b><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:402px;"><a href="/wiki/File:NMF.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/NMF.png/400px-NMF.png" decoding="async" width="400" height="100" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/NMF.png/600px-NMF.png 1.5x, //upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png 2x" data-file-width="673" data-file-height="168" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:NMF.png" class="internal" title="Enlarge"></a></div>Illustration of approximate non-negative matrix factorization: the matrix <span class="texhtml"><b>V</b></span> is represented by the two smaller matrices <span class="texhtml"><b>W</b></span> and <span class="texhtml"><b>H</b></span>, which, when multiplied, approximately reconstruct <span class="texhtml"><b>V</b></span>.</div></div></div><br /><br /><b><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Non-negative matrix factorization</a></b> (<b>NMF</b> or <b>NNMF</b>), also <b>non-negative matrix approximation</b> is a group of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> in <a href="/wiki/Multivariate_analysis" title="Multivariate analysis">multivariate analysis</a> and <a href="/wiki/Linear_algebra" title="Linear algebra">linear algebra</a> where a <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> <span class="texhtml"><b>V</b></span> is <a href="/wiki/Matrix_decomposition" title="Matrix decomposition">factorized</a> into (usually) two matrices <span class="texhtml"><b>W</b></span> and <span class="texhtml"><b>H</b></span>, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.<br /><br />NMF finds applications in such fields as <a href="/wiki/Astronomy" title="Astronomy">astronomy</a>, <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, document <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a>, <a href="/wiki/Chemometrics" title="Chemometrics">chemometrics</a>, <a href="/wiki/Audio_signal_processing" title="Audio signal processing">audio signal processing</a>, <a href="/wiki/Recommender_system" title="Recommender system">recommender systems</a>, and <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>. <b><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> is the problem of identifying to which of a set of <a href="/wiki/Categorical_data" class="mw-redirect" title="Categorical data">categories</a> (sub-populations) a new <a href="/wiki/Observation" title="Observation">observation</a> belongs, on the basis of a <a href="/wiki/Training_set" class="mw-redirect" title="Training set">training set</a> of data containing observations (or instances) whose category membership is known.  Examples are assigning a given email to the <a href="/wiki/Spam_filtering" class="mw-redirect" title="Spam filtering">"spam" or "non-spam"</a> class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).  Classification is an example of <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>.<br /><br />In the terminology of machine learning, classification is considered an instance of <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, i.e., learning where a training set of correctly identified observations is available.  The corresponding <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> procedure is known as <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a>, and involves grouping data into categories based on some measure of inherent similarity or <a href="/wiki/Distance" title="Distance">distance</a>. <b><a href="/wiki/Statistical_classification" title="Statistical classification">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Linear_regression" title="Linear regression">linear regression</a></b> is a <a href="/wiki/Linearity" title="Linearity">linear</a> approach to modelling the relationship between a scalar response (or <a href="/wiki/Dependent_variable" class="mw-redirect" title="Dependent variable">dependent variable</a>) and one or more <a href="/wiki/Explanatory_variable" class="mw-redirect" title="Explanatory variable">explanatory variables</a> (or <a href="/wiki/Independent_variable" class="mw-redirect" title="Independent variable">independent variables</a>). The case of one explanatory variable is called <a href="/wiki/Simple_linear_regression" title="Simple linear regression">simple linear regression</a>. For more than one explanatory variable, the process is called <b>multiple linear regression</b>. This term is distinct from <a href="/wiki/Multivariate_linear_regression" class="mw-redirect" title="Multivariate linear regression">multivariate linear regression</a>, where multiple correlated dependent variables are predicted, rather than a single scalar variable.<br /><br />In linear regression, the relationships are modeled using <a href="/wiki/Linear_predictor_function" title="Linear predictor function">linear predictor functions</a> whose unknown model <a href="/wiki/Parameters" class="mw-redirect" title="Parameters">parameters</a> are <a href="/wiki/Estimation_theory" title="Estimation theory">estimated</a> from the <a href="/wiki/Data" title="Data">data</a>. Such models are called <a href="/wiki/Linear_model" title="Linear model">linear models</a>. Most commonly, the <a href="/wiki/Conditional_expectation" title="Conditional expectation">conditional mean</a> of the response given the values of the explanatory variables (or predictors) is assumed to be an <a href="/wiki/Affine_transformation" title="Affine transformation">affine function</a> of those values; less commonly, the conditional <a href="/wiki/Median" title="Median">median</a> or some other <a href="/wiki/Quantile" title="Quantile">quantile</a> is used. Like all forms of <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, linear regression focuses on the <a href="/wiki/Conditional_probability_distribution" title="Conditional probability distribution">conditional probability distribution</a> of the response given the values of the predictors, rather than on the <a href="/wiki/Joint_probability_distribution" title="Joint probability distribution">joint probability distribution</a> of all of these variables, which is the domain of <a href="/wiki/Multivariate_analysis" title="Multivariate analysis">multivariate analysis</a>. <b><a href="/wiki/Linear_regression" title="Linear regression">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></b> is the process of using <a href="/wiki/Domain_knowledge" title="Domain knowledge">domain knowledge</a> of the data to create <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">features</a> that make <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated <a href="/wiki/Feature_learning" title="Feature learning">feature learning</a>.<br /><br />Feature engineering is an informal topic, but it is considered essential in applied machine learning. <b><a href="/wiki/Feature_engineering" title="Feature engineering">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision tree learning</a></b> uses a <a href="/wiki/Decision_tree" title="Decision tree">decision tree</a> (as a <a href="/wiki/Predictive_modelling" title="Predictive modelling">predictive model</a>) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in <a href="/wiki/Statistics" title="Statistics">statistics</a>, <a href="/wiki/Data_mining" title="Data mining">data mining</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>. Tree models where the target variable can take a discrete set of values are called <b>classification trees</b>; in these tree structures, <a href="/wiki/Leaf_node" class="mw-redirect" title="Leaf node">leaves</a> represent class labels and branches represent <a href="/wiki/Logical_conjunction" title="Logical conjunction">conjunctions</a> of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically <a href="/wiki/Real_numbers" class="mw-redirect" title="Real numbers">real numbers</a>) are called <b>regression trees</b>.<br /><br />In decision analysis, a decision tree can be used to visually and explicitly represent decisions and <a href="/wiki/Decision_making" class="mw-redirect" title="Decision making">decision making</a>. In <a href="/wiki/Data_mining" title="Data mining">data mining</a>, a decision tree describes data (but the resulting classification tree can be an input for <a href="/wiki/Decision_making" class="mw-redirect" title="Decision making">decision making</a>). This page deals with decision trees in <a href="/wiki/Data_mining" title="Data mining">data mining</a>. <b><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Data_mining" title="Data mining">data mining</a>, <b><a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a></b> (also <b>outlier detection</b>) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as <a href="/wiki/Bank_fraud" title="Bank fraud">bank fraud</a>, a structural defect, medical problems or errors in a text. Anomalies are also referred to as <a href="/wiki/Outlier" title="Outlier">outliers</a>, novelties, noise, deviations and exceptions.<br /><br />In particular, in the context of abuse and network intrusion detection, the interesting objects are often not <i>rare</i> objects, but unexpected <i>bursts</i> in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> algorithm may be able to detect the micro clusters formed by these patterns. <b><a href="/wiki/Anomaly_detection" title="Anomaly detection">Read more...</a></b></div>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a href="/wiki/File:Blank.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" decoding="async" width="120" height="80" data-file-width="3" data-file-height="2" /></a></div></div>
			<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">naive Bayes classifiers</a></b> are a family of simple "<a href="/wiki/Probabilistic_classifier" class="mw-redirect" title="Probabilistic classifier">probabilistic classifiers</a>" based on applying <a href="/wiki/Bayes%27_theorem" title="Bayes&#39; theorem">Bayes' theorem</a> with strong (naive) <a href="/wiki/Statistical_independence" class="mw-redirect" title="Statistical independence">independence</a> assumptions between the features.<br /><br />Naive Bayes has been studied extensively since the 1960s. It was introduced (though not under that name) into the <a href="/wiki/Information_retrieval" title="Information retrieval">text retrieval</a> community in the early 1960s, and remains a popular (baseline) method for <a href="/wiki/Text_categorization" class="mw-redirect" title="Text categorization">text categorization</a>, the problem of judging documents as belonging to one category or the other (such as <a href="/wiki/Spam_filtering" class="mw-redirect" title="Spam filtering">spam or legitimate</a>, sports or politics, etc.) with <a href="/wiki/Bag_of_words" class="mw-redirect" title="Bag of words">word frequencies</a> as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including <a href="/wiki/Support_vector_machine" class="mw-redirect" title="Support vector machine">support vector machines</a>. It also finds application in automatic <a href="/wiki/Medical_diagnosis" title="Medical diagnosis">medical diagnosis</a>. <b><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Read more...</a></b></div>
			</div>
		</div></li>

<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div>
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span id="Need_help.3F"></span><span class="mw-headline" id="Need_help?">Need help?</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<p>Do you have a question about Machine learning that you can't find the answer to?
</p><p>Consider asking it at the <a href="/wiki/Wikipedia:Reference_desk" title="Wikipedia:Reference desk">Wikipedia reference desk</a>.
</p>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div><div class="flex-columns-column"><div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Selected_images">Selected images</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<style data-mw-deduplicate="TemplateStyles:r886046910">.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div>div>span:nth-child(2){display:none}@media screen and (max-width:720px){.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li:nth-child(n/**/+5){display:none}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow{padding-left:0;padding-right:0;display:flex;flex-wrap:wrap;justify-content:space-around;align-items:flex-start}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li{width:initial!important;margin:0 0.5em}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li>div>div>div{margin:0.5em 0!important}}</style><div class="randomSlideshow-container" style="max-width:100%; margin:-4em auto;"><ul class="gallery mw-gallery-slideshow" data-showthumbnails="">
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:15px auto;"><a href="/wiki/File:Svm_max_sep_hyperplane_with_margin.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/111px-Svm_max_sep_hyperplane_with_margin.png" decoding="async" width="111" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/167px-Svm_max_sep_hyperplane_with_margin.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/223px-Svm_max_sep_hyperplane_with_margin.png 2x" data-file-width="800" data-file-height="862" /></a></div></div>
			<div class="gallerytext">
<p>A <a href="/wiki/Support_vector_machine" class="mw-redirect" title="Support vector machine">support vector machine</a> is a supervised learning model that divides the data into regions separated by a <a href="/wiki/Linear_classifier" title="Linear classifier">linear boundary</a>. Here, the linear boundary divides the black circles from the white.
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 155px"><div style="width: 155px">
			<div class="thumb" style="width: 150px;"><div style="margin:15px auto;"><a href="/wiki/File:Colored_neural_network.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/100px-Colored_neural_network.svg.png" decoding="async" width="100" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/150px-Colored_neural_network.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/199px-Colored_neural_network.svg.png 2x" data-file-width="296" data-file-height="356" /></a></div></div>
			<div class="gallerytext">
<p>An artificial neural network is an interconnected group of nodes, akin to the vast network of <a href="/wiki/Neuron" title="Neuron">neurons</a> in a <a href="/wiki/Brain" title="Brain">brain</a>. Here, each circular node represents an <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neuron</a> and an arrow represents a connection from the output of one artificial neuron to the input of another.
</p>
			</div>
		</div></li>
</ul></div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div>
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Subcategories">Subcategories</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<div class="floatright"><a href="/wiki/File:C_Puzzle.png" class="image" title="Category puzzle"><img alt="Category puzzle" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/36px-C_Puzzle.png" decoding="async" width="36" height="36" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/54px-C_Puzzle.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/72px-C_Puzzle.png 2x" data-file-width="150" data-file-height="150" /></a></div>
<dl><dd><small>Select [►] to view subcategories</small></dd></dl>
<div class="CategoryTreeTag" data-ct-mode="0" data-ct-options="{&quot;mode&quot;:0,&quot;hideprefix&quot;:20,&quot;showcount&quot;:false,&quot;namespaces&quot;:false}"><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Machine_learning" data-ct-loaded="1" data-ct-state="expanded">▼</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></div><div class="CategoryTreeChildren" style="display:block"><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Applied_machine_learning" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Applied_machine_learning" title="Category:Applied machine learning">Applied machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Artificial_neural_networks" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Bayesian_networks" title="Category:Bayesian networks">Bayesian networks</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Classification_algorithms" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Cluster_analysis" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Cluster_analysis" title="Category:Cluster analysis">Cluster analysis</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Computational_learning_theory" title="Category:Computational learning theory">Computational learning theory</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Artificial_intelligence_conferences" title="Category:Artificial intelligence conferences">Artificial intelligence conferences</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Signal_processing_conferences" title="Category:Signal processing conferences">Signal processing conferences</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Data_mining_and_machine_learning_software" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Data_mining_and_machine_learning_software" title="Category:Data mining and machine learning software">Data mining and machine learning software</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Datasets_in_machine_learning" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Datasets_in_machine_learning" title="Category:Datasets in machine learning">Datasets in machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Deep_learning" title="Category:Deep learning">Deep learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Dimension_reduction" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Ensemble_learning" title="Category:Ensemble learning">Ensemble learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Evolutionary_algorithms" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Evolutionary_algorithms" title="Category:Evolutionary algorithms">Evolutionary algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Genetic_programming" title="Category:Genetic programming">Genetic programming</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Inductive_logic_programming" title="Category:Inductive logic programming">Inductive logic programming</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Kernel_methods_for_machine_learning" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Kernel_methods_for_machine_learning" title="Category:Kernel methods for machine learning">Kernel methods for machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Latent_variable_models" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Latent_variable_models" title="Category:Latent variable models">Latent variable models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Learning_in_computer_vision" title="Category:Learning in computer vision">Learning in computer vision</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Log-linear_models" title="Category:Log-linear models">Log-linear models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Loss_functions" title="Category:Loss functions">Loss functions</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Machine_learning_algorithms" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_portal" title="Category:Machine learning portal">Machine learning portal</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_task" title="Category:Machine learning task">Machine learning task</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Markov_models" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Markov_models" title="Category:Markov models">Markov models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Ontology_learning_(computer_science)" title="Category:Ontology learning (computer science)">Ontology learning (computer science)</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_researchers" title="Category:Machine learning researchers">Machine learning researchers</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Semisupervised_learning" title="Category:Semisupervised learning">Semisupervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Statistical_natural_language_processing" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Statistical_natural_language_processing" title="Category:Statistical natural language processing">Statistical natural language processing</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-title="Structured_prediction" data-ct-state="collapsed">►</span> </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Structured_prediction" title="Category:Structured prediction">Structured prediction</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Supervised_learning" title="Category:Supervised learning">Supervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Support_vector_machines" title="Category:Support vector machines">Support vector machines</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel  CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div></div></div></div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div></div> 
<div style="clear:both; width:100%">
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Associated_Wikimedia"><i>Associated Wikimedia</i></span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<div class="noprint" style="text-align:center;">
<div style="display:inline-block;margin:0 0 0.75em 0">
<p style="margin:0">The following <a href="/wiki/Wikimedia_Foundation" title="Wikimedia Foundation">Wikimedia Foundation</a> sister projects provide more on this subject:</p>
<div style="display:inline-block;display:flex;flex-wrap:wrap;justify-content:center;"><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://en.wikibooks.org/wiki/Special:Search/Machine_learning" class="extiw" title="wikibooks:Special:Search/Machine learning">Wikibooks</a></b><br />
Books<br />
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikibooks.org/wiki/Special:Search/Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/25px-Wikibooks-logo.svg.png" decoding="async" width="25" height="25" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/38px-Wikibooks-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/50px-Wikibooks-logo.svg.png 2x" data-file-width="300" data-file-height="300" /></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://commons.wikimedia.org/wiki/Special:Search/Category:Machine_learning" class="extiw" title="commons:Special:Search/Category:Machine learning">Commons</a></b><br />
Media<br />
</p>
<div class="center"><div class="floatnone"><a href="https://commons.wikimedia.org/wiki/Special:Search/Category:Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png" decoding="async" width="18" height="25" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/28px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/37px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376" /></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://en.wikinews.org/wiki/Special:Search/Machine_learning" class="extiw" title="wikinews:Special:Search/Machine learning">Wikinews</a></b>&#160;<br />
News<br />
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikinews.org/wiki/Special:Search/Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/46px-Wikinews-logo.svg.png" decoding="async" width="46" height="25" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/70px-Wikinews-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/92px-Wikinews-logo.svg.png 2x" data-file-width="759" data-file-height="415" /></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://en.wikiquote.org/wiki/Special:Search/Machine_learning" class="extiw" title="wikiquote:Special:Search/Machine learning">Wikiquote</a></b>&#160;<br />
Quotations<br />
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikiquote.org/wiki/Special:Search/Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/21px-Wikiquote-logo.svg.png" decoding="async" width="21" height="25" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/32px-Wikiquote-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/42px-Wikiquote-logo.svg.png 2x" data-file-width="300" data-file-height="355" /></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://en.wikisource.org/wiki/Special:Search/Machine_learning" class="extiw" title="wikisource:Special:Search/Machine learning">Wikisource</a></b>&#160;<br />
Texts<br />
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikisource.org/wiki/Special:Search/Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/24px-Wikisource-logo.svg.png" decoding="async" width="24" height="25" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/36px-Wikisource-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/48px-Wikisource-logo.svg.png 2x" data-file-width="410" data-file-height="430" /></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://en.wikiversity.org/wiki/Special:Search/Machine_learning" class="extiw" title="wikiversity:Special:Search/Machine learning">Wikiversity</a></b><br />
Learning resources<br />
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikiversity.org/wiki/Special:Search/Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/31px-Wikiversity-logo.svg.png" decoding="async" width="31" height="25" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/48px-Wikiversity-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/63px-Wikiversity-logo.svg.png 2x" data-file-width="1000" data-file-height="800" /></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://en.wiktionary.org/wiki/Special:Search/Machine_learning" class="extiw" title="wiktionary:Special:Search/Machine learning">Wiktionary</a></b>&#160;<br />
Definitions<br />
</p>
<div class="center"><div class="floatnone"><a href="https://en.wiktionary.org/wiki/Special:Search/Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/25px-Wiktionary-logo-v2.svg.png" decoding="async" width="25" height="25" srcset="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/38px-Wiktionary-logo-v2.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/50px-Wiktionary-logo-v2.svg.png 2x" data-file-width="391" data-file-height="391" /></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a href="https://www.wikidata.org/wiki/Special:Search/Machine_learning" class="extiw" title="wikidata:Special:Search/Machine learning">Wikidata</a></b>&#160;<br />
Database<br />
</p>
<div class="center"><div class="floatnone"><a href="https://www.wikidata.org/wiki/Special:Search/Machine_learning"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/45px-Wikidata-logo.svg.png" decoding="async" width="45" height="25" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/68px-Wikidata-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/89px-Wikidata-logo.svg.png 2x" data-file-width="1050" data-file-height="590" /></a></div></div>
</div>
</div>
</div>
</div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div>
</div>
<div class="hlist noprint" style="text-align: center; clear:both; padding:0.25em 0 0.5em;">
<ul><li><b>What are <a href="/wiki/Wikipedia:Portal" title="Wikipedia:Portal">portals</a>?</b></li>
<li><b><a href="/wiki/Portal:Contents/Portals" title="Portal:Contents/Portals">List of portals</a></b></li></ul>
</div>
<p><i class="noprint plainlinks"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;action=purge"><small>Purge server cache</small></a></i>
</p></td></tr></tbody></table></div></div></div></li></ul></div></div></div></div></div>
<!-- 
NewPP limit report
Parsed by mw1263
Cached time: 20190305135408
Cache expiry: 21600
Dynamic content: true
CPU time usage: 4.028 seconds
Real time usage: 4.476 seconds
Preprocessor visited node count: 1157/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 70706/2097152 bytes
Template argument size: 1255/2097152 bytes
Highest expansion depth: 14/40
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 94090/5000000 bytes
Number of Wikibase entities loaded: 0/400
Lua time usage: 3.776/10.000 seconds
Lua memory usage: 5.09 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 4250.947      1 -total
 96.24% 4091.112      1 Template:Flex_columns
 42.24% 1795.610      1 Template:Transclude_selected_recent_additions
 28.57% 1214.305      1 Template:Transclude_list_item_excerpts_as_random_slideshow
 13.55%  576.025      1 Template:Transclude_files_as_random_slideshow
 10.65%  452.936      1 Template:Transclude_selected_current_events
  1.43%   60.966      8 Template:Box-header_colour
  1.10%   46.702      1 Template:Machine_learning_bar
  1.07%   45.546      1 Template:Sidebar_with_collapsible_lists
  0.91%   38.603      1 Template:Transclude_lead_excerpt
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:44942806-0!canonical!math=5 and timestamp 20190305135403 and revision id 882169711
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;oldid=882169711">https://en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;oldid=882169711</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Portals_with_short_description" title="Category:Portals with short description">Portals with short description</a></li><li><a href="/wiki/Category:Single-page_portals" title="Category:Single-page portals">Single-page portals</a></li><li><a href="/wiki/Category:All_portals" title="Category:All portals">All portals</a></li><li><a href="/wiki/Category:Portals_with_titles_not_starting_with_a_proper_noun" title="Category:Portals with titles not starting with a proper noun">Portals with titles not starting with a proper noun</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Portal%3AMachine+learning" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Portal%3AMachine+learning" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-portal" class="selected"><span><a href="/wiki/Portal:Machine_learning">Portal</a></span></li><li id="ca-talk"><span><a href="/wiki/Portal_talk:Machine_learning" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<ul class="menu">
													</ul>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Portal:Machine_learning">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Portal:Machine_learning&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Portal:Machine_learning&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<ul class="menu">
													</ul>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Portal:Machine_learning" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Portal:Machine_learning" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Portal:Machine_learning&amp;oldid=882169711" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Portal:Machine_learning&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q58630879" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Portal%3AMachine+learning">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Portal%3AMachine+learning&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Portal:Machine_learning&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D8%A8%D9%88%D8%A7%D8%A8%D8%A9:%D8%A7%D9%84%D8%AA%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A2%D9%84%D9%8A" title="بوابة:التعلم الآلي – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Portail:Donn%C3%A9es/Machine_Learning" title="Portail:Données/Machine Learning – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q58630879#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 7 February 2019, at 07:59<span class="anonymous-show">&#160;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"4.028","walltime":"4.476","ppvisitednodes":{"value":1157,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":70706,"limit":2097152},"templateargumentsize":{"value":1255,"limit":2097152},"expansiondepth":{"value":14,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":94090,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00% 4250.947      1 -total"," 96.24% 4091.112      1 Template:Flex_columns"," 42.24% 1795.610      1 Template:Transclude_selected_recent_additions"," 28.57% 1214.305      1 Template:Transclude_list_item_excerpts_as_random_slideshow"," 13.55%  576.025      1 Template:Transclude_files_as_random_slideshow"," 10.65%  452.936      1 Template:Transclude_selected_current_events","  1.43%   60.966      8 Template:Box-header_colour","  1.10%   46.702      1 Template:Machine_learning_bar","  1.07%   45.546      1 Template:Sidebar_with_collapsible_lists","  0.91%   38.603      1 Template:Transclude_lead_excerpt"]},"scribunto":{"limitreport-timeusage":{"value":"3.776","limit":"10.000"},"limitreport-memusage":{"value":5333942,"limit":52428800},"limitreport-logs":"table#1 {\n  [\"size\"] = \"tiny\",\n}\n"},"cachereport":{"origin":"mw1263","timestamp":"20190305135408","ttl":21600,"transientcontent":true}}});mw.config.set({"wgBackendResponseTime":95,"wgHostname":"mw1328"});});</script>
	</body>
</html>
